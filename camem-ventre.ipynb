{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.2.0 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (1.2.0+cu92)\n",
      "Requirement already satisfied: torchvision==0.4.0 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (0.4.0+cu92)\n",
      "Requirement already satisfied: numpy in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from torch==1.2.0) (1.17.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from torchvision==0.4.0) (7.0.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from torchvision==0.4.0) (1.11.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/export/data/swisspost/enviroments/word_embeddings/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: seaborn in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (0.10.0)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from seaborn) (3.1.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from seaborn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from seaborn) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from seaborn) (1.17.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from pandas>=0.22.0->seaborn) (2019.3)\n",
      "Requirement already satisfied: setuptools in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.2->seaborn) (41.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.2->seaborn) (1.11.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/export/data/swisspost/enviroments/word_embeddings/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.2.0 torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification, CamembertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX TITAN X\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\", 0)\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(1))\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN = 'abdominal'\n",
    "PATH = '20200410'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv('training_data/simplify_label/ellipsis_training.csv')\n",
    "df = pd.read_csv(PATH+'/{0}_compactExpansion_200410.txt'.format(DOMAIN), sep=\";\", names=['canonical', 'variation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.variation.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "preprocess = lambda sent: re.sub(r'\\s+', ' ', re.sub('-', ' ', re.sub(\"'\", \"' \",  sent.strip())))\n",
    "preprocess_canonical = lambda sent: re.sub(r'\\s+', ' ', sent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df.variations = df.variation.apply(preprocess)\n",
    "df.canonical = df.canonical.apply(preprocess_canonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonical</th>\n",
       "      <th>variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avez-vous mal au ventre ?</td>\n",
       "      <td>vous êtes venu parce que vous aviez cette doul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avez-vous mal au ventre ?</td>\n",
       "      <td>on ma dit que vous aviez  mal à l'abdomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avez-vous mal au ventre ?</td>\n",
       "      <td>est-ce que vous avez noté ce mal abdominale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avez-vous mal au ventre ?</td>\n",
       "      <td>on m'a dit que vous aviez un mal à l'abdomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avez-vous mal au ventre ?</td>\n",
       "      <td>cela vous fait-il  maux de ventre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218761</th>\n",
       "      <td>avez-vous senti une masse dans les testicules ?</td>\n",
       "      <td>vous vous êtes déjà palpé une boule au niveau ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218762</th>\n",
       "      <td>avez-vous senti une masse dans les testicules ?</td>\n",
       "      <td>avez-vous noté une masse au niveau du testicule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218763</th>\n",
       "      <td>avez-vous senti une masse dans les testicules ?</td>\n",
       "      <td>avez-vous palpé une masse dans les testicules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218764</th>\n",
       "      <td>avez-vous senti une masse dans les testicules ?</td>\n",
       "      <td>vous arrive-t-il d'éprouver une masse au nivea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218765</th>\n",
       "      <td>avez-vous senti une masse dans les testicules ?</td>\n",
       "      <td>vous arrive-t-il d'avoir une masse au niveau d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>218607 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              canonical  \\\n",
       "0                             avez-vous mal au ventre ?   \n",
       "1                             avez-vous mal au ventre ?   \n",
       "2                             avez-vous mal au ventre ?   \n",
       "3                             avez-vous mal au ventre ?   \n",
       "4                             avez-vous mal au ventre ?   \n",
       "...                                                 ...   \n",
       "218761  avez-vous senti une masse dans les testicules ?   \n",
       "218762  avez-vous senti une masse dans les testicules ?   \n",
       "218763  avez-vous senti une masse dans les testicules ?   \n",
       "218764  avez-vous senti une masse dans les testicules ?   \n",
       "218765  avez-vous senti une masse dans les testicules ?   \n",
       "\n",
       "                                                variation  \n",
       "0       vous êtes venu parce que vous aviez cette doul...  \n",
       "1               on ma dit que vous aviez  mal à l'abdomen  \n",
       "2             est-ce que vous avez noté ce mal abdominale  \n",
       "3            on m'a dit que vous aviez un mal à l'abdomen  \n",
       "4                       cela vous fait-il  maux de ventre  \n",
       "...                                                   ...  \n",
       "218761  vous vous êtes déjà palpé une boule au niveau ...  \n",
       "218762    avez-vous noté une masse au niveau du testicule  \n",
       "218763      avez-vous palpé une masse dans les testicules  \n",
       "218764  vous arrive-t-il d'éprouver une masse au nivea...  \n",
       "218765  vous arrive-t-il d'avoir une masse au niveau d...  \n",
       "\n",
       "[218607 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_model(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2canonical = df.canonical.unique()\n",
    "canonical2index = dict([(canonical, i) for (i, canonical) in enumerate(index2canonical)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isdir(os.path.join(PATH, DOMAIN)):\n",
    "    os.mkdir(os.path.join(PATH, DOMAIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(index2canonical, PATH+'/{0}/index2canonical'.format(DOMAIN))\n",
    "save_model(canonical2index, PATH+'/{0}/canonical2index'.format(DOMAIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.canonical.apply(lambda sent: canonical2index[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.variation.values\n",
    "y = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218607"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cambertTokenizer = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  vous arrive-t-il d'avoir une masse au niveau du testicule\n",
      "Token IDs: [5, 39, 1242, 26, 110, 26, 62, 18, 11, 443, 28, 2269, 36, 359, 25, 2006, 14194, 6]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in X:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = cambertTokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "                        # This function also supports truncation and conversion\n",
    "                        # to pytorch tensors, but we need to do padding, so we\n",
    "                        # can't use these features :( .\n",
    "                        #max_length = 128,          # Truncate all sentences.\n",
    "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# print last sentence, now as a list of IDs.\n",
    "print('Original: ', X[-1])\n",
    "print('Token IDs:', input_ids[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  39\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# We'll borrow the `pad_sequences` utility function to do this.\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"<pad>\", ID: 1\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# max length sentence + 5. For test data.\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "print('\\nPadding/truncating all sentences to {} values...'.format(MAX_LEN))\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(cambertTokenizer.pad_token, cambertTokenizer.pad_token_id))\n",
    "\n",
    "# Pad our input tokens with value 0.\n",
    "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
    "# as opposed to the beginning.\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for\n",
    "# training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, y, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "\n",
    "# Do the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, y,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CamembertForSequenceClassification.from_pretrained('camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = len(index2canonical), # The number of output labels for multi-class classification.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3107, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model on GPU\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs \n",
    "epochs = 15\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 15 ========\n",
      "Training...\n",
      "  Batch    40  of  7,870.    Elapsed: 0:00:05.\n",
      "  Batch    80  of  7,870.    Elapsed: 0:00:10.\n",
      "  Batch   120  of  7,870.    Elapsed: 0:00:15.\n",
      "  Batch   160  of  7,870.    Elapsed: 0:00:19.\n",
      "  Batch   200  of  7,870.    Elapsed: 0:00:24.\n",
      "  Batch   240  of  7,870.    Elapsed: 0:00:29.\n",
      "  Batch   280  of  7,870.    Elapsed: 0:00:34.\n",
      "  Batch   320  of  7,870.    Elapsed: 0:00:39.\n",
      "  Batch   360  of  7,870.    Elapsed: 0:00:44.\n",
      "  Batch   400  of  7,870.    Elapsed: 0:00:49.\n",
      "  Batch   440  of  7,870.    Elapsed: 0:00:54.\n",
      "  Batch   480  of  7,870.    Elapsed: 0:00:59.\n",
      "  Batch   520  of  7,870.    Elapsed: 0:01:03.\n",
      "  Batch   560  of  7,870.    Elapsed: 0:01:08.\n",
      "  Batch   600  of  7,870.    Elapsed: 0:01:13.\n",
      "  Batch   640  of  7,870.    Elapsed: 0:01:18.\n",
      "  Batch   680  of  7,870.    Elapsed: 0:01:23.\n",
      "  Batch   720  of  7,870.    Elapsed: 0:01:28.\n",
      "  Batch   760  of  7,870.    Elapsed: 0:01:33.\n",
      "  Batch   800  of  7,870.    Elapsed: 0:01:38.\n",
      "  Batch   840  of  7,870.    Elapsed: 0:01:43.\n",
      "  Batch   880  of  7,870.    Elapsed: 0:01:47.\n",
      "  Batch   920  of  7,870.    Elapsed: 0:01:52.\n",
      "  Batch   960  of  7,870.    Elapsed: 0:01:57.\n",
      "  Batch 1,000  of  7,870.    Elapsed: 0:02:02.\n",
      "  Batch 1,040  of  7,870.    Elapsed: 0:02:07.\n",
      "  Batch 1,080  of  7,870.    Elapsed: 0:02:12.\n",
      "  Batch 1,120  of  7,870.    Elapsed: 0:02:17.\n",
      "  Batch 1,160  of  7,870.    Elapsed: 0:02:22.\n",
      "  Batch 1,200  of  7,870.    Elapsed: 0:02:27.\n",
      "  Batch 1,240  of  7,870.    Elapsed: 0:02:32.\n",
      "  Batch 1,280  of  7,870.    Elapsed: 0:02:37.\n",
      "  Batch 1,320  of  7,870.    Elapsed: 0:02:42.\n",
      "  Batch 1,360  of  7,870.    Elapsed: 0:02:47.\n",
      "  Batch 1,400  of  7,870.    Elapsed: 0:02:52.\n",
      "  Batch 1,440  of  7,870.    Elapsed: 0:02:57.\n",
      "  Batch 1,480  of  7,870.    Elapsed: 0:03:02.\n",
      "  Batch 1,520  of  7,870.    Elapsed: 0:03:07.\n",
      "  Batch 1,560  of  7,870.    Elapsed: 0:03:12.\n",
      "  Batch 1,600  of  7,870.    Elapsed: 0:03:17.\n",
      "  Batch 1,640  of  7,870.    Elapsed: 0:03:22.\n",
      "  Batch 1,680  of  7,870.    Elapsed: 0:03:28.\n",
      "  Batch 1,720  of  7,870.    Elapsed: 0:03:33.\n",
      "  Batch 1,760  of  7,870.    Elapsed: 0:03:38.\n",
      "  Batch 1,800  of  7,870.    Elapsed: 0:03:43.\n",
      "  Batch 1,840  of  7,870.    Elapsed: 0:03:48.\n",
      "  Batch 1,880  of  7,870.    Elapsed: 0:03:53.\n",
      "  Batch 1,920  of  7,870.    Elapsed: 0:03:59.\n",
      "  Batch 1,960  of  7,870.    Elapsed: 0:04:04.\n",
      "  Batch 2,000  of  7,870.    Elapsed: 0:04:09.\n",
      "  Batch 2,040  of  7,870.    Elapsed: 0:04:14.\n",
      "  Batch 2,080  of  7,870.    Elapsed: 0:04:19.\n",
      "  Batch 2,120  of  7,870.    Elapsed: 0:04:25.\n",
      "  Batch 2,160  of  7,870.    Elapsed: 0:04:30.\n",
      "  Batch 2,200  of  7,870.    Elapsed: 0:04:35.\n",
      "  Batch 2,240  of  7,870.    Elapsed: 0:04:40.\n",
      "  Batch 2,280  of  7,870.    Elapsed: 0:04:46.\n",
      "  Batch 2,320  of  7,870.    Elapsed: 0:04:51.\n",
      "  Batch 2,360  of  7,870.    Elapsed: 0:04:56.\n",
      "  Batch 2,400  of  7,870.    Elapsed: 0:05:01.\n",
      "  Batch 2,440  of  7,870.    Elapsed: 0:05:07.\n",
      "  Batch 2,480  of  7,870.    Elapsed: 0:05:12.\n",
      "  Batch 2,520  of  7,870.    Elapsed: 0:05:17.\n",
      "  Batch 2,560  of  7,870.    Elapsed: 0:05:22.\n",
      "  Batch 2,600  of  7,870.    Elapsed: 0:05:27.\n",
      "  Batch 2,640  of  7,870.    Elapsed: 0:05:33.\n",
      "  Batch 2,680  of  7,870.    Elapsed: 0:05:38.\n",
      "  Batch 2,720  of  7,870.    Elapsed: 0:05:43.\n",
      "  Batch 2,760  of  7,870.    Elapsed: 0:05:48.\n",
      "  Batch 2,800  of  7,870.    Elapsed: 0:05:54.\n",
      "  Batch 2,840  of  7,870.    Elapsed: 0:05:59.\n",
      "  Batch 2,880  of  7,870.    Elapsed: 0:06:04.\n",
      "  Batch 2,920  of  7,870.    Elapsed: 0:06:09.\n",
      "  Batch 2,960  of  7,870.    Elapsed: 0:06:15.\n",
      "  Batch 3,000  of  7,870.    Elapsed: 0:06:20.\n",
      "  Batch 3,040  of  7,870.    Elapsed: 0:06:25.\n",
      "  Batch 3,080  of  7,870.    Elapsed: 0:06:30.\n",
      "  Batch 3,120  of  7,870.    Elapsed: 0:06:35.\n",
      "  Batch 3,160  of  7,870.    Elapsed: 0:06:41.\n",
      "  Batch 3,200  of  7,870.    Elapsed: 0:06:46.\n",
      "  Batch 3,240  of  7,870.    Elapsed: 0:06:51.\n",
      "  Batch 3,280  of  7,870.    Elapsed: 0:06:56.\n",
      "  Batch 3,320  of  7,870.    Elapsed: 0:07:01.\n",
      "  Batch 3,360  of  7,870.    Elapsed: 0:07:07.\n",
      "  Batch 3,400  of  7,870.    Elapsed: 0:07:12.\n",
      "  Batch 3,440  of  7,870.    Elapsed: 0:07:17.\n",
      "  Batch 3,480  of  7,870.    Elapsed: 0:07:22.\n",
      "  Batch 3,520  of  7,870.    Elapsed: 0:07:27.\n",
      "  Batch 3,560  of  7,870.    Elapsed: 0:07:33.\n",
      "  Batch 3,600  of  7,870.    Elapsed: 0:07:38.\n",
      "  Batch 3,640  of  7,870.    Elapsed: 0:07:43.\n",
      "  Batch 3,680  of  7,870.    Elapsed: 0:07:48.\n",
      "  Batch 3,720  of  7,870.    Elapsed: 0:07:53.\n",
      "  Batch 3,760  of  7,870.    Elapsed: 0:07:58.\n",
      "  Batch 3,800  of  7,870.    Elapsed: 0:08:04.\n",
      "  Batch 3,840  of  7,870.    Elapsed: 0:08:09.\n",
      "  Batch 3,880  of  7,870.    Elapsed: 0:08:14.\n",
      "  Batch 3,920  of  7,870.    Elapsed: 0:08:19.\n",
      "  Batch 3,960  of  7,870.    Elapsed: 0:08:25.\n",
      "  Batch 4,000  of  7,870.    Elapsed: 0:08:30.\n",
      "  Batch 4,040  of  7,870.    Elapsed: 0:08:35.\n",
      "  Batch 4,080  of  7,870.    Elapsed: 0:08:40.\n",
      "  Batch 4,120  of  7,870.    Elapsed: 0:08:45.\n",
      "  Batch 4,160  of  7,870.    Elapsed: 0:08:51.\n",
      "  Batch 4,200  of  7,870.    Elapsed: 0:08:56.\n",
      "  Batch 4,240  of  7,870.    Elapsed: 0:09:01.\n",
      "  Batch 4,280  of  7,870.    Elapsed: 0:09:06.\n",
      "  Batch 4,320  of  7,870.    Elapsed: 0:09:11.\n",
      "  Batch 4,360  of  7,870.    Elapsed: 0:09:16.\n",
      "  Batch 4,400  of  7,870.    Elapsed: 0:09:22.\n",
      "  Batch 4,440  of  7,870.    Elapsed: 0:09:27.\n",
      "  Batch 4,480  of  7,870.    Elapsed: 0:09:32.\n",
      "  Batch 4,520  of  7,870.    Elapsed: 0:09:37.\n",
      "  Batch 4,560  of  7,870.    Elapsed: 0:09:42.\n",
      "  Batch 4,600  of  7,870.    Elapsed: 0:09:48.\n",
      "  Batch 4,640  of  7,870.    Elapsed: 0:09:53.\n",
      "  Batch 4,680  of  7,870.    Elapsed: 0:09:58.\n",
      "  Batch 4,720  of  7,870.    Elapsed: 0:10:03.\n",
      "  Batch 4,760  of  7,870.    Elapsed: 0:10:08.\n",
      "  Batch 4,800  of  7,870.    Elapsed: 0:10:14.\n",
      "  Batch 4,840  of  7,870.    Elapsed: 0:10:19.\n",
      "  Batch 4,880  of  7,870.    Elapsed: 0:10:24.\n",
      "  Batch 4,920  of  7,870.    Elapsed: 0:10:29.\n",
      "  Batch 4,960  of  7,870.    Elapsed: 0:10:34.\n",
      "  Batch 5,000  of  7,870.    Elapsed: 0:10:39.\n",
      "  Batch 5,040  of  7,870.    Elapsed: 0:10:45.\n",
      "  Batch 5,080  of  7,870.    Elapsed: 0:10:50.\n",
      "  Batch 5,120  of  7,870.    Elapsed: 0:10:55.\n",
      "  Batch 5,160  of  7,870.    Elapsed: 0:11:00.\n",
      "  Batch 5,200  of  7,870.    Elapsed: 0:11:05.\n",
      "  Batch 5,240  of  7,870.    Elapsed: 0:11:11.\n",
      "  Batch 5,280  of  7,870.    Elapsed: 0:11:16.\n",
      "  Batch 5,320  of  7,870.    Elapsed: 0:11:21.\n",
      "  Batch 5,360  of  7,870.    Elapsed: 0:11:26.\n",
      "  Batch 5,400  of  7,870.    Elapsed: 0:11:31.\n",
      "  Batch 5,440  of  7,870.    Elapsed: 0:11:37.\n",
      "  Batch 5,480  of  7,870.    Elapsed: 0:11:42.\n",
      "  Batch 5,520  of  7,870.    Elapsed: 0:11:47.\n",
      "  Batch 5,560  of  7,870.    Elapsed: 0:11:52.\n",
      "  Batch 5,600  of  7,870.    Elapsed: 0:11:57.\n",
      "  Batch 5,640  of  7,870.    Elapsed: 0:12:03.\n",
      "  Batch 5,680  of  7,870.    Elapsed: 0:12:08.\n",
      "  Batch 5,720  of  7,870.    Elapsed: 0:12:13.\n",
      "  Batch 5,760  of  7,870.    Elapsed: 0:12:18.\n",
      "  Batch 5,800  of  7,870.    Elapsed: 0:12:23.\n",
      "  Batch 5,840  of  7,870.    Elapsed: 0:12:28.\n",
      "  Batch 5,880  of  7,870.    Elapsed: 0:12:34.\n",
      "  Batch 5,920  of  7,870.    Elapsed: 0:12:39.\n",
      "  Batch 5,960  of  7,870.    Elapsed: 0:12:44.\n",
      "  Batch 6,000  of  7,870.    Elapsed: 0:12:49.\n",
      "  Batch 6,040  of  7,870.    Elapsed: 0:12:55.\n",
      "  Batch 6,080  of  7,870.    Elapsed: 0:13:00.\n",
      "  Batch 6,120  of  7,870.    Elapsed: 0:13:05.\n",
      "  Batch 6,160  of  7,870.    Elapsed: 0:13:10.\n",
      "  Batch 6,200  of  7,870.    Elapsed: 0:13:15.\n",
      "  Batch 6,240  of  7,870.    Elapsed: 0:13:20.\n",
      "  Batch 6,280  of  7,870.    Elapsed: 0:13:26.\n",
      "  Batch 6,320  of  7,870.    Elapsed: 0:13:31.\n",
      "  Batch 6,360  of  7,870.    Elapsed: 0:13:36.\n",
      "  Batch 6,400  of  7,870.    Elapsed: 0:13:41.\n",
      "  Batch 6,440  of  7,870.    Elapsed: 0:13:46.\n",
      "  Batch 6,480  of  7,870.    Elapsed: 0:13:52.\n",
      "  Batch 6,520  of  7,870.    Elapsed: 0:13:57.\n",
      "  Batch 6,560  of  7,870.    Elapsed: 0:14:02.\n",
      "  Batch 6,600  of  7,870.    Elapsed: 0:14:07.\n",
      "  Batch 6,640  of  7,870.    Elapsed: 0:14:12.\n",
      "  Batch 6,680  of  7,870.    Elapsed: 0:14:18.\n",
      "  Batch 6,720  of  7,870.    Elapsed: 0:14:23.\n",
      "  Batch 6,760  of  7,870.    Elapsed: 0:14:28.\n",
      "  Batch 6,800  of  7,870.    Elapsed: 0:14:33.\n",
      "  Batch 6,840  of  7,870.    Elapsed: 0:14:38.\n",
      "  Batch 6,880  of  7,870.    Elapsed: 0:14:44.\n",
      "  Batch 6,920  of  7,870.    Elapsed: 0:14:49.\n",
      "  Batch 6,960  of  7,870.    Elapsed: 0:14:54.\n",
      "  Batch 7,000  of  7,870.    Elapsed: 0:14:59.\n",
      "  Batch 7,040  of  7,870.    Elapsed: 0:15:04.\n",
      "  Batch 7,080  of  7,870.    Elapsed: 0:15:09.\n",
      "  Batch 7,120  of  7,870.    Elapsed: 0:15:15.\n",
      "  Batch 7,160  of  7,870.    Elapsed: 0:15:20.\n",
      "  Batch 7,200  of  7,870.    Elapsed: 0:15:25.\n",
      "  Batch 7,240  of  7,870.    Elapsed: 0:15:30.\n",
      "  Batch 7,280  of  7,870.    Elapsed: 0:15:35.\n",
      "  Batch 7,320  of  7,870.    Elapsed: 0:15:41.\n",
      "  Batch 7,360  of  7,870.    Elapsed: 0:15:46.\n",
      "  Batch 7,400  of  7,870.    Elapsed: 0:15:51.\n",
      "  Batch 7,440  of  7,870.    Elapsed: 0:15:56.\n",
      "  Batch 7,480  of  7,870.    Elapsed: 0:16:01.\n",
      "  Batch 7,520  of  7,870.    Elapsed: 0:16:07.\n",
      "  Batch 7,560  of  7,870.    Elapsed: 0:16:12.\n",
      "  Batch 7,600  of  7,870.    Elapsed: 0:16:17.\n",
      "  Batch 7,640  of  7,870.    Elapsed: 0:16:22.\n",
      "  Batch 7,680  of  7,870.    Elapsed: 0:16:27.\n",
      "  Batch 7,720  of  7,870.    Elapsed: 0:16:33.\n",
      "  Batch 7,760  of  7,870.    Elapsed: 0:16:38.\n",
      "  Batch 7,800  of  7,870.    Elapsed: 0:16:43.\n",
      "  Batch 7,840  of  7,870.    Elapsed: 0:16:48.\n",
      "\n",
      "  Average training loss: 5.83\n",
      "  Training epcoh took: 0:16:52\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.57\n",
      "  Validation took: 0:00:28\n",
      "\n",
      "======== Epoch 2 / 15 ========\n",
      "Training...\n",
      "  Batch    40  of  7,870.    Elapsed: 0:00:05.\n",
      "  Batch    80  of  7,870.    Elapsed: 0:00:10.\n",
      "  Batch   120  of  7,870.    Elapsed: 0:00:16.\n",
      "  Batch   160  of  7,870.    Elapsed: 0:00:21.\n",
      "  Batch   200  of  7,870.    Elapsed: 0:00:26.\n",
      "  Batch   240  of  7,870.    Elapsed: 0:00:31.\n",
      "  Batch   280  of  7,870.    Elapsed: 0:00:36.\n",
      "  Batch   320  of  7,870.    Elapsed: 0:00:42.\n",
      "  Batch   360  of  7,870.    Elapsed: 0:00:47.\n",
      "  Batch   400  of  7,870.    Elapsed: 0:00:52.\n",
      "  Batch   440  of  7,870.    Elapsed: 0:00:57.\n",
      "  Batch   480  of  7,870.    Elapsed: 0:01:02.\n",
      "  Batch   520  of  7,870.    Elapsed: 0:01:08.\n",
      "  Batch   560  of  7,870.    Elapsed: 0:01:13.\n",
      "  Batch   600  of  7,870.    Elapsed: 0:01:18.\n",
      "  Batch   640  of  7,870.    Elapsed: 0:01:23.\n",
      "  Batch   680  of  7,870.    Elapsed: 0:01:28.\n",
      "  Batch   720  of  7,870.    Elapsed: 0:01:34.\n",
      "  Batch   760  of  7,870.    Elapsed: 0:01:39.\n",
      "  Batch   800  of  7,870.    Elapsed: 0:01:44.\n",
      "  Batch   840  of  7,870.    Elapsed: 0:01:49.\n",
      "  Batch   880  of  7,870.    Elapsed: 0:01:54.\n",
      "  Batch   920  of  7,870.    Elapsed: 0:02:00.\n",
      "  Batch   960  of  7,870.    Elapsed: 0:02:05.\n",
      "  Batch 1,000  of  7,870.    Elapsed: 0:02:10.\n",
      "  Batch 1,040  of  7,870.    Elapsed: 0:02:15.\n",
      "  Batch 1,080  of  7,870.    Elapsed: 0:02:20.\n",
      "  Batch 1,120  of  7,870.    Elapsed: 0:02:26.\n",
      "  Batch 1,160  of  7,870.    Elapsed: 0:02:31.\n",
      "  Batch 1,200  of  7,870.    Elapsed: 0:02:36.\n",
      "  Batch 1,240  of  7,870.    Elapsed: 0:02:41.\n",
      "  Batch 1,280  of  7,870.    Elapsed: 0:02:46.\n",
      "  Batch 1,320  of  7,870.    Elapsed: 0:02:52.\n",
      "  Batch 1,360  of  7,870.    Elapsed: 0:02:57.\n",
      "  Batch 1,400  of  7,870.    Elapsed: 0:03:02.\n",
      "  Batch 1,440  of  7,870.    Elapsed: 0:03:07.\n",
      "  Batch 1,480  of  7,870.    Elapsed: 0:03:12.\n",
      "  Batch 1,520  of  7,870.    Elapsed: 0:03:18.\n",
      "  Batch 1,560  of  7,870.    Elapsed: 0:03:23.\n",
      "  Batch 1,600  of  7,870.    Elapsed: 0:03:28.\n",
      "  Batch 1,640  of  7,870.    Elapsed: 0:03:33.\n",
      "  Batch 1,680  of  7,870.    Elapsed: 0:03:38.\n",
      "  Batch 1,720  of  7,870.    Elapsed: 0:03:44.\n",
      "  Batch 1,760  of  7,870.    Elapsed: 0:03:49.\n",
      "  Batch 1,800  of  7,870.    Elapsed: 0:03:54.\n",
      "  Batch 1,840  of  7,870.    Elapsed: 0:03:59.\n",
      "  Batch 1,880  of  7,870.    Elapsed: 0:04:04.\n",
      "  Batch 1,920  of  7,870.    Elapsed: 0:04:10.\n",
      "  Batch 1,960  of  7,870.    Elapsed: 0:04:15.\n",
      "  Batch 2,000  of  7,870.    Elapsed: 0:04:20.\n",
      "  Batch 2,040  of  7,870.    Elapsed: 0:04:25.\n",
      "  Batch 2,080  of  7,870.    Elapsed: 0:04:30.\n",
      "  Batch 2,120  of  7,870.    Elapsed: 0:04:36.\n",
      "  Batch 2,160  of  7,870.    Elapsed: 0:04:41.\n",
      "  Batch 2,200  of  7,870.    Elapsed: 0:04:46.\n",
      "  Batch 2,240  of  7,870.    Elapsed: 0:04:51.\n",
      "  Batch 2,280  of  7,870.    Elapsed: 0:04:56.\n",
      "  Batch 2,320  of  7,870.    Elapsed: 0:05:02.\n",
      "  Batch 2,360  of  7,870.    Elapsed: 0:05:07.\n",
      "  Batch 2,400  of  7,870.    Elapsed: 0:05:12.\n",
      "  Batch 2,440  of  7,870.    Elapsed: 0:05:17.\n",
      "  Batch 2,480  of  7,870.    Elapsed: 0:05:22.\n",
      "  Batch 2,520  of  7,870.    Elapsed: 0:05:28.\n",
      "  Batch 2,560  of  7,870.    Elapsed: 0:05:33.\n",
      "  Batch 2,600  of  7,870.    Elapsed: 0:05:38.\n",
      "  Batch 2,640  of  7,870.    Elapsed: 0:05:43.\n",
      "  Batch 2,680  of  7,870.    Elapsed: 0:05:49.\n",
      "  Batch 2,720  of  7,870.    Elapsed: 0:05:54.\n",
      "  Batch 2,760  of  7,870.    Elapsed: 0:05:59.\n",
      "  Batch 2,800  of  7,870.    Elapsed: 0:06:04.\n",
      "  Batch 2,840  of  7,870.    Elapsed: 0:06:09.\n",
      "  Batch 2,880  of  7,870.    Elapsed: 0:06:15.\n",
      "  Batch 2,920  of  7,870.    Elapsed: 0:06:20.\n",
      "  Batch 2,960  of  7,870.    Elapsed: 0:06:25.\n",
      "  Batch 3,000  of  7,870.    Elapsed: 0:06:30.\n",
      "  Batch 3,040  of  7,870.    Elapsed: 0:06:35.\n",
      "  Batch 3,080  of  7,870.    Elapsed: 0:06:41.\n",
      "  Batch 3,120  of  7,870.    Elapsed: 0:06:46.\n",
      "  Batch 3,160  of  7,870.    Elapsed: 0:06:51.\n",
      "  Batch 3,200  of  7,870.    Elapsed: 0:06:56.\n",
      "  Batch 3,240  of  7,870.    Elapsed: 0:07:01.\n",
      "  Batch 3,280  of  7,870.    Elapsed: 0:07:07.\n",
      "  Batch 3,320  of  7,870.    Elapsed: 0:07:12.\n",
      "  Batch 3,360  of  7,870.    Elapsed: 0:07:17.\n",
      "  Batch 3,400  of  7,870.    Elapsed: 0:07:22.\n",
      "  Batch 3,440  of  7,870.    Elapsed: 0:07:27.\n",
      "  Batch 3,480  of  7,870.    Elapsed: 0:07:33.\n",
      "  Batch 3,520  of  7,870.    Elapsed: 0:07:38.\n",
      "  Batch 3,560  of  7,870.    Elapsed: 0:07:43.\n",
      "  Batch 3,600  of  7,870.    Elapsed: 0:07:48.\n",
      "  Batch 3,640  of  7,870.    Elapsed: 0:07:53.\n",
      "  Batch 3,680  of  7,870.    Elapsed: 0:07:59.\n",
      "  Batch 3,720  of  7,870.    Elapsed: 0:08:04.\n",
      "  Batch 3,760  of  7,870.    Elapsed: 0:08:09.\n",
      "  Batch 3,800  of  7,870.    Elapsed: 0:08:14.\n",
      "  Batch 3,840  of  7,870.    Elapsed: 0:08:19.\n",
      "  Batch 3,880  of  7,870.    Elapsed: 0:08:25.\n",
      "  Batch 3,920  of  7,870.    Elapsed: 0:08:30.\n",
      "  Batch 3,960  of  7,870.    Elapsed: 0:08:35.\n",
      "  Batch 4,000  of  7,870.    Elapsed: 0:08:40.\n",
      "  Batch 4,040  of  7,870.    Elapsed: 0:08:46.\n",
      "  Batch 4,080  of  7,870.    Elapsed: 0:08:51.\n",
      "  Batch 4,120  of  7,870.    Elapsed: 0:08:56.\n",
      "  Batch 4,160  of  7,870.    Elapsed: 0:09:01.\n",
      "  Batch 4,200  of  7,870.    Elapsed: 0:09:06.\n",
      "  Batch 4,240  of  7,870.    Elapsed: 0:09:12.\n",
      "  Batch 4,280  of  7,870.    Elapsed: 0:09:17.\n",
      "  Batch 4,320  of  7,870.    Elapsed: 0:09:22.\n",
      "  Batch 4,360  of  7,870.    Elapsed: 0:09:27.\n",
      "  Batch 4,400  of  7,870.    Elapsed: 0:09:33.\n",
      "  Batch 4,440  of  7,870.    Elapsed: 0:09:38.\n",
      "  Batch 4,480  of  7,870.    Elapsed: 0:09:43.\n",
      "  Batch 4,520  of  7,870.    Elapsed: 0:09:48.\n",
      "  Batch 4,560  of  7,870.    Elapsed: 0:09:53.\n",
      "  Batch 4,600  of  7,870.    Elapsed: 0:09:58.\n",
      "  Batch 4,640  of  7,870.    Elapsed: 0:10:03.\n",
      "  Batch 4,680  of  7,870.    Elapsed: 0:10:08.\n",
      "  Batch 4,720  of  7,870.    Elapsed: 0:10:13.\n",
      "  Batch 4,760  of  7,870.    Elapsed: 0:10:18.\n",
      "  Batch 4,800  of  7,870.    Elapsed: 0:10:23.\n",
      "  Batch 4,840  of  7,870.    Elapsed: 0:10:28.\n",
      "  Batch 4,880  of  7,870.    Elapsed: 0:10:33.\n",
      "  Batch 4,920  of  7,870.    Elapsed: 0:10:38.\n",
      "  Batch 4,960  of  7,870.    Elapsed: 0:10:43.\n",
      "  Batch 5,000  of  7,870.    Elapsed: 0:10:48.\n",
      "  Batch 5,040  of  7,870.    Elapsed: 0:10:53.\n",
      "  Batch 5,080  of  7,870.    Elapsed: 0:10:58.\n",
      "  Batch 5,120  of  7,870.    Elapsed: 0:11:03.\n",
      "  Batch 5,160  of  7,870.    Elapsed: 0:11:08.\n",
      "  Batch 5,200  of  7,870.    Elapsed: 0:11:13.\n",
      "  Batch 5,240  of  7,870.    Elapsed: 0:11:18.\n",
      "  Batch 5,280  of  7,870.    Elapsed: 0:11:23.\n",
      "  Batch 5,320  of  7,870.    Elapsed: 0:11:28.\n",
      "  Batch 5,360  of  7,870.    Elapsed: 0:11:33.\n",
      "  Batch 5,400  of  7,870.    Elapsed: 0:11:38.\n",
      "  Batch 5,440  of  7,870.    Elapsed: 0:11:43.\n",
      "  Batch 5,480  of  7,870.    Elapsed: 0:11:48.\n",
      "  Batch 5,520  of  7,870.    Elapsed: 0:11:53.\n",
      "  Batch 5,560  of  7,870.    Elapsed: 0:11:58.\n",
      "  Batch 5,600  of  7,870.    Elapsed: 0:12:03.\n",
      "  Batch 5,640  of  7,870.    Elapsed: 0:12:08.\n",
      "  Batch 5,680  of  7,870.    Elapsed: 0:12:13.\n",
      "  Batch 5,720  of  7,870.    Elapsed: 0:12:18.\n",
      "  Batch 5,760  of  7,870.    Elapsed: 0:12:23.\n",
      "  Batch 5,800  of  7,870.    Elapsed: 0:12:28.\n",
      "  Batch 5,840  of  7,870.    Elapsed: 0:12:33.\n",
      "  Batch 5,880  of  7,870.    Elapsed: 0:12:38.\n",
      "  Batch 5,920  of  7,870.    Elapsed: 0:12:43.\n",
      "  Batch 5,960  of  7,870.    Elapsed: 0:12:48.\n",
      "  Batch 6,000  of  7,870.    Elapsed: 0:12:53.\n",
      "  Batch 6,040  of  7,870.    Elapsed: 0:12:58.\n",
      "  Batch 6,080  of  7,870.    Elapsed: 0:13:03.\n",
      "  Batch 6,120  of  7,870.    Elapsed: 0:13:08.\n",
      "  Batch 6,160  of  7,870.    Elapsed: 0:13:12.\n",
      "  Batch 6,200  of  7,870.    Elapsed: 0:13:17.\n",
      "  Batch 6,240  of  7,870.    Elapsed: 0:13:22.\n",
      "  Batch 6,280  of  7,870.    Elapsed: 0:13:27.\n",
      "  Batch 6,320  of  7,870.    Elapsed: 0:13:32.\n",
      "  Batch 6,360  of  7,870.    Elapsed: 0:13:37.\n",
      "  Batch 6,400  of  7,870.    Elapsed: 0:13:42.\n",
      "  Batch 6,440  of  7,870.    Elapsed: 0:13:47.\n",
      "  Batch 6,480  of  7,870.    Elapsed: 0:13:52.\n",
      "  Batch 6,520  of  7,870.    Elapsed: 0:13:57.\n",
      "  Batch 6,560  of  7,870.    Elapsed: 0:14:02.\n",
      "  Batch 6,600  of  7,870.    Elapsed: 0:14:07.\n",
      "  Batch 6,640  of  7,870.    Elapsed: 0:14:12.\n",
      "  Batch 6,680  of  7,870.    Elapsed: 0:14:17.\n",
      "  Batch 6,720  of  7,870.    Elapsed: 0:14:22.\n",
      "  Batch 6,760  of  7,870.    Elapsed: 0:14:27.\n",
      "  Batch 6,800  of  7,870.    Elapsed: 0:14:32.\n",
      "  Batch 6,840  of  7,870.    Elapsed: 0:14:37.\n",
      "  Batch 6,880  of  7,870.    Elapsed: 0:14:42.\n",
      "  Batch 6,920  of  7,870.    Elapsed: 0:14:47.\n",
      "  Batch 6,960  of  7,870.    Elapsed: 0:14:52.\n",
      "  Batch 7,000  of  7,870.    Elapsed: 0:14:57.\n",
      "  Batch 7,040  of  7,870.    Elapsed: 0:15:02.\n",
      "  Batch 7,080  of  7,870.    Elapsed: 0:15:07.\n",
      "  Batch 7,120  of  7,870.    Elapsed: 0:15:12.\n",
      "  Batch 7,160  of  7,870.    Elapsed: 0:15:17.\n",
      "  Batch 7,200  of  7,870.    Elapsed: 0:15:22.\n",
      "  Batch 7,240  of  7,870.    Elapsed: 0:15:27.\n",
      "  Batch 7,280  of  7,870.    Elapsed: 0:15:32.\n",
      "  Batch 7,320  of  7,870.    Elapsed: 0:15:37.\n",
      "  Batch 7,360  of  7,870.    Elapsed: 0:15:42.\n",
      "  Batch 7,400  of  7,870.    Elapsed: 0:15:47.\n",
      "  Batch 7,440  of  7,870.    Elapsed: 0:15:52.\n",
      "  Batch 7,480  of  7,870.    Elapsed: 0:15:57.\n",
      "  Batch 7,520  of  7,870.    Elapsed: 0:16:02.\n",
      "  Batch 7,560  of  7,870.    Elapsed: 0:16:07.\n",
      "  Batch 7,600  of  7,870.    Elapsed: 0:16:12.\n",
      "  Batch 7,640  of  7,870.    Elapsed: 0:16:17.\n",
      "  Batch 7,680  of  7,870.    Elapsed: 0:16:22.\n",
      "  Batch 7,720  of  7,870.    Elapsed: 0:16:27.\n",
      "  Batch 7,760  of  7,870.    Elapsed: 0:16:32.\n",
      "  Batch 7,800  of  7,870.    Elapsed: 0:16:37.\n",
      "  Batch 7,840  of  7,870.    Elapsed: 0:16:42.\n",
      "\n",
      "  Average training loss: 2.05\n",
      "  Training epcoh took: 0:16:46\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation took: 0:00:28\n",
      "\n",
      "======== Epoch 3 / 15 ========\n",
      "Training...\n",
      "  Batch    40  of  7,870.    Elapsed: 0:00:05.\n",
      "  Batch    80  of  7,870.    Elapsed: 0:00:10.\n",
      "  Batch   120  of  7,870.    Elapsed: 0:00:15.\n",
      "  Batch   160  of  7,870.    Elapsed: 0:00:20.\n",
      "  Batch   200  of  7,870.    Elapsed: 0:00:25.\n",
      "  Batch   240  of  7,870.    Elapsed: 0:00:30.\n",
      "  Batch   280  of  7,870.    Elapsed: 0:00:35.\n",
      "  Batch   320  of  7,870.    Elapsed: 0:00:40.\n",
      "  Batch   360  of  7,870.    Elapsed: 0:00:45.\n",
      "  Batch   400  of  7,870.    Elapsed: 0:00:50.\n",
      "  Batch   440  of  7,870.    Elapsed: 0:00:55.\n",
      "  Batch   480  of  7,870.    Elapsed: 0:01:00.\n",
      "  Batch   520  of  7,870.    Elapsed: 0:01:05.\n",
      "  Batch   560  of  7,870.    Elapsed: 0:01:10.\n",
      "  Batch   600  of  7,870.    Elapsed: 0:01:15.\n",
      "  Batch   640  of  7,870.    Elapsed: 0:01:20.\n",
      "  Batch   680  of  7,870.    Elapsed: 0:01:25.\n",
      "  Batch   720  of  7,870.    Elapsed: 0:01:30.\n",
      "  Batch   760  of  7,870.    Elapsed: 0:01:35.\n",
      "  Batch   800  of  7,870.    Elapsed: 0:01:40.\n",
      "  Batch   840  of  7,870.    Elapsed: 0:01:45.\n",
      "  Batch   880  of  7,870.    Elapsed: 0:01:50.\n",
      "  Batch   920  of  7,870.    Elapsed: 0:01:55.\n",
      "  Batch   960  of  7,870.    Elapsed: 0:02:00.\n",
      "  Batch 1,000  of  7,870.    Elapsed: 0:02:05.\n",
      "  Batch 1,040  of  7,870.    Elapsed: 0:02:10.\n",
      "  Batch 1,080  of  7,870.    Elapsed: 0:02:15.\n",
      "  Batch 1,120  of  7,870.    Elapsed: 0:02:20.\n",
      "  Batch 1,160  of  7,870.    Elapsed: 0:02:25.\n",
      "  Batch 1,200  of  7,870.    Elapsed: 0:02:30.\n",
      "  Batch 1,240  of  7,870.    Elapsed: 0:02:35.\n",
      "  Batch 1,280  of  7,870.    Elapsed: 0:02:40.\n",
      "  Batch 1,320  of  7,870.    Elapsed: 0:02:45.\n",
      "  Batch 1,360  of  7,870.    Elapsed: 0:02:50.\n",
      "  Batch 1,400  of  7,870.    Elapsed: 0:02:55.\n",
      "  Batch 1,440  of  7,870.    Elapsed: 0:03:00.\n",
      "  Batch 1,480  of  7,870.    Elapsed: 0:03:05.\n",
      "  Batch 1,520  of  7,870.    Elapsed: 0:03:10.\n",
      "  Batch 1,560  of  7,870.    Elapsed: 0:03:15.\n",
      "  Batch 1,600  of  7,870.    Elapsed: 0:03:20.\n",
      "  Batch 1,640  of  7,870.    Elapsed: 0:03:25.\n",
      "  Batch 1,680  of  7,870.    Elapsed: 0:03:30.\n",
      "  Batch 1,720  of  7,870.    Elapsed: 0:03:35.\n",
      "  Batch 1,760  of  7,870.    Elapsed: 0:03:40.\n",
      "  Batch 1,800  of  7,870.    Elapsed: 0:03:45.\n",
      "  Batch 1,840  of  7,870.    Elapsed: 0:03:50.\n",
      "  Batch 1,880  of  7,870.    Elapsed: 0:03:55.\n",
      "  Batch 1,920  of  7,870.    Elapsed: 0:04:00.\n",
      "  Batch 1,960  of  7,870.    Elapsed: 0:04:05.\n",
      "  Batch 2,000  of  7,870.    Elapsed: 0:04:10.\n",
      "  Batch 2,040  of  7,870.    Elapsed: 0:04:15.\n",
      "  Batch 2,080  of  7,870.    Elapsed: 0:04:20.\n",
      "  Batch 2,120  of  7,870.    Elapsed: 0:04:25.\n",
      "  Batch 2,160  of  7,870.    Elapsed: 0:04:30.\n",
      "  Batch 2,200  of  7,870.    Elapsed: 0:04:35.\n",
      "  Batch 2,240  of  7,870.    Elapsed: 0:04:40.\n",
      "  Batch 2,280  of  7,870.    Elapsed: 0:04:45.\n",
      "  Batch 2,320  of  7,870.    Elapsed: 0:04:50.\n",
      "  Batch 2,360  of  7,870.    Elapsed: 0:04:55.\n",
      "  Batch 2,400  of  7,870.    Elapsed: 0:05:00.\n",
      "  Batch 2,440  of  7,870.    Elapsed: 0:05:05.\n",
      "  Batch 2,480  of  7,870.    Elapsed: 0:05:10.\n",
      "  Batch 2,520  of  7,870.    Elapsed: 0:05:15.\n",
      "  Batch 2,560  of  7,870.    Elapsed: 0:05:20.\n",
      "  Batch 2,600  of  7,870.    Elapsed: 0:05:25.\n",
      "  Batch 2,640  of  7,870.    Elapsed: 0:05:30.\n",
      "  Batch 2,680  of  7,870.    Elapsed: 0:05:35.\n",
      "  Batch 2,720  of  7,870.    Elapsed: 0:05:40.\n",
      "  Batch 2,760  of  7,870.    Elapsed: 0:05:45.\n",
      "  Batch 2,800  of  7,870.    Elapsed: 0:05:50.\n",
      "  Batch 2,840  of  7,870.    Elapsed: 0:05:55.\n",
      "  Batch 2,880  of  7,870.    Elapsed: 0:06:00.\n",
      "  Batch 2,920  of  7,870.    Elapsed: 0:06:05.\n",
      "  Batch 2,960  of  7,870.    Elapsed: 0:06:10.\n",
      "  Batch 3,000  of  7,870.    Elapsed: 0:06:14.\n",
      "  Batch 3,040  of  7,870.    Elapsed: 0:06:19.\n",
      "  Batch 3,080  of  7,870.    Elapsed: 0:06:24.\n",
      "  Batch 3,120  of  7,870.    Elapsed: 0:06:29.\n",
      "  Batch 3,160  of  7,870.    Elapsed: 0:06:34.\n",
      "  Batch 3,200  of  7,870.    Elapsed: 0:06:39.\n",
      "  Batch 3,240  of  7,870.    Elapsed: 0:06:44.\n",
      "  Batch 3,280  of  7,870.    Elapsed: 0:06:49.\n",
      "  Batch 3,320  of  7,870.    Elapsed: 0:06:54.\n",
      "  Batch 3,360  of  7,870.    Elapsed: 0:06:59.\n",
      "  Batch 3,400  of  7,870.    Elapsed: 0:07:04.\n",
      "  Batch 3,440  of  7,870.    Elapsed: 0:07:09.\n",
      "  Batch 3,480  of  7,870.    Elapsed: 0:07:14.\n",
      "  Batch 3,520  of  7,870.    Elapsed: 0:07:19.\n",
      "  Batch 3,560  of  7,870.    Elapsed: 0:07:24.\n",
      "  Batch 3,600  of  7,870.    Elapsed: 0:07:29.\n",
      "  Batch 3,640  of  7,870.    Elapsed: 0:07:34.\n",
      "  Batch 3,680  of  7,870.    Elapsed: 0:07:39.\n",
      "  Batch 3,720  of  7,870.    Elapsed: 0:07:44.\n",
      "  Batch 3,760  of  7,870.    Elapsed: 0:07:49.\n",
      "  Batch 3,800  of  7,870.    Elapsed: 0:07:54.\n",
      "  Batch 3,840  of  7,870.    Elapsed: 0:07:59.\n",
      "  Batch 3,880  of  7,870.    Elapsed: 0:08:04.\n",
      "  Batch 3,920  of  7,870.    Elapsed: 0:08:09.\n",
      "  Batch 3,960  of  7,870.    Elapsed: 0:08:14.\n",
      "  Batch 4,000  of  7,870.    Elapsed: 0:08:19.\n",
      "  Batch 4,040  of  7,870.    Elapsed: 0:08:24.\n",
      "  Batch 4,080  of  7,870.    Elapsed: 0:08:29.\n",
      "  Batch 4,120  of  7,870.    Elapsed: 0:08:34.\n",
      "  Batch 4,160  of  7,870.    Elapsed: 0:08:39.\n",
      "  Batch 4,200  of  7,870.    Elapsed: 0:08:44.\n",
      "  Batch 4,240  of  7,870.    Elapsed: 0:08:49.\n",
      "  Batch 4,280  of  7,870.    Elapsed: 0:08:54.\n",
      "  Batch 4,320  of  7,870.    Elapsed: 0:08:59.\n",
      "  Batch 4,360  of  7,870.    Elapsed: 0:09:04.\n",
      "  Batch 4,400  of  7,870.    Elapsed: 0:09:09.\n",
      "  Batch 4,440  of  7,870.    Elapsed: 0:09:14.\n",
      "  Batch 4,480  of  7,870.    Elapsed: 0:09:19.\n",
      "  Batch 4,520  of  7,870.    Elapsed: 0:09:24.\n",
      "  Batch 4,560  of  7,870.    Elapsed: 0:09:29.\n",
      "  Batch 4,600  of  7,870.    Elapsed: 0:09:34.\n",
      "  Batch 4,640  of  7,870.    Elapsed: 0:09:39.\n",
      "  Batch 4,680  of  7,870.    Elapsed: 0:09:44.\n",
      "  Batch 4,720  of  7,870.    Elapsed: 0:09:49.\n",
      "  Batch 4,760  of  7,870.    Elapsed: 0:09:54.\n",
      "  Batch 4,800  of  7,870.    Elapsed: 0:09:59.\n",
      "  Batch 4,840  of  7,870.    Elapsed: 0:10:04.\n",
      "  Batch 4,880  of  7,870.    Elapsed: 0:10:09.\n",
      "  Batch 4,920  of  7,870.    Elapsed: 0:10:14.\n",
      "  Batch 4,960  of  7,870.    Elapsed: 0:10:19.\n",
      "  Batch 5,000  of  7,870.    Elapsed: 0:10:24.\n",
      "  Batch 5,040  of  7,870.    Elapsed: 0:10:29.\n",
      "  Batch 5,080  of  7,870.    Elapsed: 0:10:34.\n",
      "  Batch 5,120  of  7,870.    Elapsed: 0:10:39.\n",
      "  Batch 5,160  of  7,870.    Elapsed: 0:10:44.\n",
      "  Batch 5,200  of  7,870.    Elapsed: 0:10:49.\n",
      "  Batch 5,240  of  7,870.    Elapsed: 0:10:54.\n",
      "  Batch 5,280  of  7,870.    Elapsed: 0:10:59.\n",
      "  Batch 5,320  of  7,870.    Elapsed: 0:11:04.\n",
      "  Batch 5,360  of  7,870.    Elapsed: 0:11:09.\n",
      "  Batch 5,400  of  7,870.    Elapsed: 0:11:14.\n",
      "  Batch 5,440  of  7,870.    Elapsed: 0:11:19.\n",
      "  Batch 5,480  of  7,870.    Elapsed: 0:11:24.\n",
      "  Batch 5,520  of  7,870.    Elapsed: 0:11:29.\n",
      "  Batch 5,560  of  7,870.    Elapsed: 0:11:34.\n",
      "  Batch 5,600  of  7,870.    Elapsed: 0:11:39.\n",
      "  Batch 5,640  of  7,870.    Elapsed: 0:11:44.\n",
      "  Batch 5,680  of  7,870.    Elapsed: 0:11:49.\n",
      "  Batch 5,720  of  7,870.    Elapsed: 0:11:54.\n",
      "  Batch 5,760  of  7,870.    Elapsed: 0:11:59.\n",
      "  Batch 5,800  of  7,870.    Elapsed: 0:12:04.\n",
      "  Batch 5,840  of  7,870.    Elapsed: 0:12:09.\n",
      "  Batch 5,880  of  7,870.    Elapsed: 0:12:14.\n",
      "  Batch 5,920  of  7,870.    Elapsed: 0:12:19.\n",
      "  Batch 5,960  of  7,870.    Elapsed: 0:12:24.\n",
      "  Batch 6,000  of  7,870.    Elapsed: 0:12:29.\n",
      "  Batch 6,040  of  7,870.    Elapsed: 0:12:34.\n",
      "  Batch 6,080  of  7,870.    Elapsed: 0:12:39.\n",
      "  Batch 6,120  of  7,870.    Elapsed: 0:12:44.\n",
      "  Batch 6,160  of  7,870.    Elapsed: 0:12:49.\n",
      "  Batch 6,200  of  7,870.    Elapsed: 0:12:54.\n",
      "  Batch 6,240  of  7,870.    Elapsed: 0:12:59.\n",
      "  Batch 6,280  of  7,870.    Elapsed: 0:13:04.\n",
      "  Batch 6,320  of  7,870.    Elapsed: 0:13:09.\n",
      "  Batch 6,360  of  7,870.    Elapsed: 0:13:14.\n",
      "  Batch 6,400  of  7,870.    Elapsed: 0:13:19.\n",
      "  Batch 6,440  of  7,870.    Elapsed: 0:13:24.\n",
      "  Batch 6,480  of  7,870.    Elapsed: 0:13:29.\n",
      "  Batch 6,520  of  7,870.    Elapsed: 0:13:34.\n",
      "  Batch 6,560  of  7,870.    Elapsed: 0:13:39.\n",
      "  Batch 6,600  of  7,870.    Elapsed: 0:13:44.\n",
      "  Batch 6,640  of  7,870.    Elapsed: 0:13:49.\n",
      "  Batch 6,680  of  7,870.    Elapsed: 0:13:54.\n",
      "  Batch 6,720  of  7,870.    Elapsed: 0:13:59.\n",
      "  Batch 6,760  of  7,870.    Elapsed: 0:14:04.\n",
      "  Batch 6,800  of  7,870.    Elapsed: 0:14:09.\n",
      "  Batch 6,840  of  7,870.    Elapsed: 0:14:14.\n",
      "  Batch 6,880  of  7,870.    Elapsed: 0:14:19.\n",
      "  Batch 6,920  of  7,870.    Elapsed: 0:14:24.\n",
      "  Batch 6,960  of  7,870.    Elapsed: 0:14:29.\n",
      "  Batch 7,000  of  7,870.    Elapsed: 0:14:34.\n",
      "  Batch 7,040  of  7,870.    Elapsed: 0:14:39.\n",
      "  Batch 7,080  of  7,870.    Elapsed: 0:14:44.\n",
      "  Batch 7,120  of  7,870.    Elapsed: 0:14:49.\n",
      "  Batch 7,160  of  7,870.    Elapsed: 0:14:54.\n",
      "  Batch 7,200  of  7,870.    Elapsed: 0:14:59.\n",
      "  Batch 7,240  of  7,870.    Elapsed: 0:15:04.\n",
      "  Batch 7,280  of  7,870.    Elapsed: 0:15:09.\n",
      "  Batch 7,320  of  7,870.    Elapsed: 0:15:14.\n",
      "  Batch 7,360  of  7,870.    Elapsed: 0:15:19.\n",
      "  Batch 7,400  of  7,870.    Elapsed: 0:15:24.\n",
      "  Batch 7,440  of  7,870.    Elapsed: 0:15:29.\n",
      "  Batch 7,480  of  7,870.    Elapsed: 0:15:34.\n",
      "  Batch 7,520  of  7,870.    Elapsed: 0:15:39.\n",
      "  Batch 7,560  of  7,870.    Elapsed: 0:15:44.\n",
      "  Batch 7,600  of  7,870.    Elapsed: 0:15:49.\n",
      "  Batch 7,640  of  7,870.    Elapsed: 0:15:54.\n",
      "  Batch 7,680  of  7,870.    Elapsed: 0:15:59.\n",
      "  Batch 7,720  of  7,870.    Elapsed: 0:16:04.\n",
      "  Batch 7,760  of  7,870.    Elapsed: 0:16:10.\n",
      "  Batch 7,800  of  7,870.    Elapsed: 0:16:15.\n",
      "  Batch 7,840  of  7,870.    Elapsed: 0:16:20.\n",
      "\n",
      "  Average training loss: 0.90\n",
      "  Training epcoh took: 0:16:23\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation took: 0:00:29\n",
      "\n",
      "======== Epoch 4 / 15 ========\n",
      "Training...\n",
      "  Batch    40  of  7,870.    Elapsed: 0:00:05.\n",
      "  Batch    80  of  7,870.    Elapsed: 0:00:10.\n",
      "  Batch   120  of  7,870.    Elapsed: 0:00:15.\n",
      "  Batch   160  of  7,870.    Elapsed: 0:00:20.\n",
      "  Batch   200  of  7,870.    Elapsed: 0:00:25.\n",
      "  Batch   240  of  7,870.    Elapsed: 0:00:30.\n",
      "  Batch   280  of  7,870.    Elapsed: 0:00:35.\n",
      "  Batch   320  of  7,870.    Elapsed: 0:00:40.\n",
      "  Batch   360  of  7,870.    Elapsed: 0:00:45.\n",
      "  Batch   400  of  7,870.    Elapsed: 0:00:50.\n",
      "  Batch   440  of  7,870.    Elapsed: 0:00:55.\n",
      "  Batch   480  of  7,870.    Elapsed: 0:01:00.\n",
      "  Batch   520  of  7,870.    Elapsed: 0:01:05.\n",
      "  Batch   560  of  7,870.    Elapsed: 0:01:10.\n",
      "  Batch   600  of  7,870.    Elapsed: 0:01:15.\n",
      "  Batch   640  of  7,870.    Elapsed: 0:01:20.\n",
      "  Batch   680  of  7,870.    Elapsed: 0:01:25.\n",
      "  Batch   720  of  7,870.    Elapsed: 0:01:30.\n",
      "  Batch   760  of  7,870.    Elapsed: 0:01:35.\n",
      "  Batch   800  of  7,870.    Elapsed: 0:01:40.\n",
      "  Batch   840  of  7,870.    Elapsed: 0:01:45.\n",
      "  Batch   880  of  7,870.    Elapsed: 0:01:50.\n",
      "  Batch   920  of  7,870.    Elapsed: 0:01:55.\n",
      "  Batch   960  of  7,870.    Elapsed: 0:02:00.\n",
      "  Batch 1,000  of  7,870.    Elapsed: 0:02:05.\n",
      "  Batch 1,040  of  7,870.    Elapsed: 0:02:10.\n",
      "  Batch 1,080  of  7,870.    Elapsed: 0:02:15.\n",
      "  Batch 1,120  of  7,870.    Elapsed: 0:02:20.\n",
      "  Batch 1,160  of  7,870.    Elapsed: 0:02:25.\n",
      "  Batch 1,200  of  7,870.    Elapsed: 0:02:30.\n",
      "  Batch 1,240  of  7,870.    Elapsed: 0:02:35.\n",
      "  Batch 1,280  of  7,870.    Elapsed: 0:02:40.\n",
      "  Batch 1,320  of  7,870.    Elapsed: 0:02:45.\n",
      "  Batch 1,360  of  7,870.    Elapsed: 0:02:50.\n",
      "  Batch 1,400  of  7,870.    Elapsed: 0:02:55.\n",
      "  Batch 1,440  of  7,870.    Elapsed: 0:03:00.\n",
      "  Batch 1,480  of  7,870.    Elapsed: 0:03:05.\n",
      "  Batch 1,520  of  7,870.    Elapsed: 0:03:10.\n",
      "  Batch 1,560  of  7,870.    Elapsed: 0:03:15.\n",
      "  Batch 1,600  of  7,870.    Elapsed: 0:03:20.\n",
      "  Batch 1,640  of  7,870.    Elapsed: 0:03:25.\n",
      "  Batch 1,680  of  7,870.    Elapsed: 0:03:30.\n",
      "  Batch 1,720  of  7,870.    Elapsed: 0:03:35.\n",
      "  Batch 1,760  of  7,870.    Elapsed: 0:03:40.\n",
      "  Batch 1,800  of  7,870.    Elapsed: 0:03:46.\n",
      "  Batch 1,840  of  7,870.    Elapsed: 0:03:51.\n",
      "  Batch 1,880  of  7,870.    Elapsed: 0:03:56.\n",
      "  Batch 1,920  of  7,870.    Elapsed: 0:04:01.\n",
      "  Batch 1,960  of  7,870.    Elapsed: 0:04:06.\n",
      "  Batch 2,000  of  7,870.    Elapsed: 0:04:11.\n",
      "  Batch 2,040  of  7,870.    Elapsed: 0:04:16.\n",
      "  Batch 2,080  of  7,870.    Elapsed: 0:04:21.\n",
      "  Batch 2,120  of  7,870.    Elapsed: 0:04:26.\n",
      "  Batch 2,160  of  7,870.    Elapsed: 0:04:31.\n",
      "  Batch 2,200  of  7,870.    Elapsed: 0:04:36.\n",
      "  Batch 2,240  of  7,870.    Elapsed: 0:04:41.\n",
      "  Batch 2,280  of  7,870.    Elapsed: 0:04:46.\n",
      "  Batch 2,320  of  7,870.    Elapsed: 0:04:51.\n",
      "  Batch 2,360  of  7,870.    Elapsed: 0:04:56.\n",
      "  Batch 2,400  of  7,870.    Elapsed: 0:05:01.\n",
      "  Batch 2,440  of  7,870.    Elapsed: 0:05:06.\n",
      "  Batch 2,480  of  7,870.    Elapsed: 0:05:11.\n",
      "  Batch 2,520  of  7,870.    Elapsed: 0:05:16.\n",
      "  Batch 2,560  of  7,870.    Elapsed: 0:05:21.\n",
      "  Batch 2,600  of  7,870.    Elapsed: 0:05:26.\n",
      "  Batch 2,640  of  7,870.    Elapsed: 0:05:31.\n",
      "  Batch 2,680  of  7,870.    Elapsed: 0:05:36.\n",
      "  Batch 2,720  of  7,870.    Elapsed: 0:05:41.\n",
      "  Batch 2,760  of  7,870.    Elapsed: 0:05:46.\n",
      "  Batch 2,800  of  7,870.    Elapsed: 0:05:51.\n",
      "  Batch 2,840  of  7,870.    Elapsed: 0:05:56.\n",
      "  Batch 2,880  of  7,870.    Elapsed: 0:06:01.\n",
      "  Batch 2,920  of  7,870.    Elapsed: 0:06:06.\n",
      "  Batch 2,960  of  7,870.    Elapsed: 0:06:11.\n",
      "  Batch 3,000  of  7,870.    Elapsed: 0:06:16.\n",
      "  Batch 3,040  of  7,870.    Elapsed: 0:06:21.\n",
      "  Batch 3,080  of  7,870.    Elapsed: 0:06:26.\n",
      "  Batch 3,120  of  7,870.    Elapsed: 0:06:31.\n",
      "  Batch 3,160  of  7,870.    Elapsed: 0:06:36.\n",
      "  Batch 3,200  of  7,870.    Elapsed: 0:06:41.\n",
      "  Batch 3,240  of  7,870.    Elapsed: 0:06:46.\n",
      "  Batch 3,280  of  7,870.    Elapsed: 0:06:51.\n",
      "  Batch 3,320  of  7,870.    Elapsed: 0:06:56.\n",
      "  Batch 3,360  of  7,870.    Elapsed: 0:07:01.\n",
      "  Batch 3,400  of  7,870.    Elapsed: 0:07:06.\n",
      "  Batch 3,440  of  7,870.    Elapsed: 0:07:11.\n",
      "  Batch 3,480  of  7,870.    Elapsed: 0:07:16.\n",
      "  Batch 3,520  of  7,870.    Elapsed: 0:07:21.\n",
      "  Batch 3,560  of  7,870.    Elapsed: 0:07:26.\n",
      "  Batch 3,600  of  7,870.    Elapsed: 0:07:31.\n",
      "  Batch 3,640  of  7,870.    Elapsed: 0:07:36.\n",
      "  Batch 3,680  of  7,870.    Elapsed: 0:07:41.\n",
      "  Batch 3,720  of  7,870.    Elapsed: 0:07:46.\n",
      "  Batch 3,760  of  7,870.    Elapsed: 0:07:51.\n",
      "  Batch 3,800  of  7,870.    Elapsed: 0:07:56.\n",
      "  Batch 3,840  of  7,870.    Elapsed: 0:08:01.\n",
      "  Batch 3,880  of  7,870.    Elapsed: 0:08:06.\n",
      "  Batch 3,920  of  7,870.    Elapsed: 0:08:11.\n",
      "  Batch 3,960  of  7,870.    Elapsed: 0:08:16.\n",
      "  Batch 4,000  of  7,870.    Elapsed: 0:08:21.\n",
      "  Batch 4,040  of  7,870.    Elapsed: 0:08:26.\n",
      "  Batch 4,080  of  7,870.    Elapsed: 0:08:31.\n",
      "  Batch 4,120  of  7,870.    Elapsed: 0:08:36.\n",
      "  Batch 4,160  of  7,870.    Elapsed: 0:08:41.\n",
      "  Batch 4,200  of  7,870.    Elapsed: 0:08:46.\n",
      "  Batch 4,240  of  7,870.    Elapsed: 0:08:51.\n",
      "  Batch 4,280  of  7,870.    Elapsed: 0:08:56.\n",
      "  Batch 4,320  of  7,870.    Elapsed: 0:09:01.\n",
      "  Batch 4,360  of  7,870.    Elapsed: 0:09:06.\n",
      "  Batch 4,400  of  7,870.    Elapsed: 0:09:11.\n",
      "  Batch 4,440  of  7,870.    Elapsed: 0:09:16.\n",
      "  Batch 4,480  of  7,870.    Elapsed: 0:09:21.\n",
      "  Batch 4,520  of  7,870.    Elapsed: 0:09:26.\n",
      "  Batch 4,560  of  7,870.    Elapsed: 0:09:31.\n",
      "  Batch 4,600  of  7,870.    Elapsed: 0:09:36.\n",
      "  Batch 4,640  of  7,870.    Elapsed: 0:09:41.\n",
      "  Batch 4,680  of  7,870.    Elapsed: 0:09:46.\n",
      "  Batch 4,720  of  7,870.    Elapsed: 0:09:51.\n",
      "  Batch 4,760  of  7,870.    Elapsed: 0:09:56.\n",
      "  Batch 4,800  of  7,870.    Elapsed: 0:10:01.\n",
      "  Batch 4,840  of  7,870.    Elapsed: 0:10:06.\n",
      "  Batch 4,880  of  7,870.    Elapsed: 0:10:11.\n",
      "  Batch 4,920  of  7,870.    Elapsed: 0:10:16.\n",
      "  Batch 4,960  of  7,870.    Elapsed: 0:10:21.\n",
      "  Batch 5,000  of  7,870.    Elapsed: 0:10:26.\n",
      "  Batch 5,040  of  7,870.    Elapsed: 0:10:31.\n",
      "  Batch 5,080  of  7,870.    Elapsed: 0:10:36.\n",
      "  Batch 5,120  of  7,870.    Elapsed: 0:10:41.\n",
      "  Batch 5,160  of  7,870.    Elapsed: 0:10:46.\n",
      "  Batch 5,200  of  7,870.    Elapsed: 0:10:51.\n",
      "  Batch 5,240  of  7,870.    Elapsed: 0:10:56.\n",
      "  Batch 5,280  of  7,870.    Elapsed: 0:11:01.\n",
      "  Batch 5,320  of  7,870.    Elapsed: 0:11:07.\n",
      "  Batch 5,360  of  7,870.    Elapsed: 0:11:12.\n",
      "  Batch 5,400  of  7,870.    Elapsed: 0:11:17.\n",
      "  Batch 5,440  of  7,870.    Elapsed: 0:11:22.\n",
      "  Batch 5,480  of  7,870.    Elapsed: 0:11:27.\n",
      "  Batch 5,520  of  7,870.    Elapsed: 0:11:32.\n",
      "  Batch 5,560  of  7,870.    Elapsed: 0:11:37.\n",
      "  Batch 5,600  of  7,870.    Elapsed: 0:11:42.\n",
      "  Batch 5,640  of  7,870.    Elapsed: 0:11:47.\n",
      "  Batch 5,680  of  7,870.    Elapsed: 0:11:52.\n",
      "  Batch 5,720  of  7,870.    Elapsed: 0:11:57.\n",
      "  Batch 5,760  of  7,870.    Elapsed: 0:12:03.\n",
      "  Batch 5,800  of  7,870.    Elapsed: 0:12:08.\n",
      "  Batch 5,840  of  7,870.    Elapsed: 0:12:13.\n",
      "  Batch 5,880  of  7,870.    Elapsed: 0:12:18.\n",
      "  Batch 5,920  of  7,870.    Elapsed: 0:12:24.\n",
      "  Batch 5,960  of  7,870.    Elapsed: 0:12:29.\n",
      "  Batch 6,000  of  7,870.    Elapsed: 0:12:34.\n",
      "  Batch 6,040  of  7,870.    Elapsed: 0:12:39.\n",
      "  Batch 6,080  of  7,870.    Elapsed: 0:12:44.\n",
      "  Batch 6,120  of  7,870.    Elapsed: 0:12:50.\n",
      "  Batch 6,160  of  7,870.    Elapsed: 0:12:55.\n",
      "  Batch 6,200  of  7,870.    Elapsed: 0:13:00.\n",
      "  Batch 6,240  of  7,870.    Elapsed: 0:13:05.\n",
      "  Batch 6,280  of  7,870.    Elapsed: 0:13:11.\n",
      "  Batch 6,320  of  7,870.    Elapsed: 0:13:16.\n",
      "  Batch 6,360  of  7,870.    Elapsed: 0:13:21.\n",
      "  Batch 6,400  of  7,870.    Elapsed: 0:13:26.\n",
      "  Batch 6,440  of  7,870.    Elapsed: 0:13:32.\n",
      "  Batch 6,480  of  7,870.    Elapsed: 0:13:37.\n",
      "  Batch 6,520  of  7,870.    Elapsed: 0:13:42.\n",
      "  Batch 6,560  of  7,870.    Elapsed: 0:13:47.\n",
      "  Batch 6,600  of  7,870.    Elapsed: 0:13:53.\n",
      "  Batch 6,640  of  7,870.    Elapsed: 0:13:58.\n",
      "  Batch 6,680  of  7,870.    Elapsed: 0:14:03.\n",
      "  Batch 6,720  of  7,870.    Elapsed: 0:14:08.\n",
      "  Batch 6,760  of  7,870.    Elapsed: 0:14:14.\n",
      "  Batch 6,800  of  7,870.    Elapsed: 0:14:19.\n",
      "  Batch 6,840  of  7,870.    Elapsed: 0:14:24.\n",
      "  Batch 6,880  of  7,870.    Elapsed: 0:14:29.\n",
      "  Batch 6,920  of  7,870.    Elapsed: 0:14:34.\n",
      "  Batch 6,960  of  7,870.    Elapsed: 0:14:40.\n",
      "  Batch 7,000  of  7,870.    Elapsed: 0:14:45.\n",
      "  Batch 7,040  of  7,870.    Elapsed: 0:14:50.\n",
      "  Batch 7,080  of  7,870.    Elapsed: 0:14:55.\n",
      "  Batch 7,120  of  7,870.    Elapsed: 0:15:01.\n",
      "  Batch 7,160  of  7,870.    Elapsed: 0:15:06.\n",
      "  Batch 7,200  of  7,870.    Elapsed: 0:15:11.\n",
      "  Batch 7,240  of  7,870.    Elapsed: 0:15:16.\n",
      "  Batch 7,280  of  7,870.    Elapsed: 0:15:21.\n",
      "  Batch 7,320  of  7,870.    Elapsed: 0:15:27.\n",
      "  Batch 7,360  of  7,870.    Elapsed: 0:15:32.\n",
      "  Batch 7,400  of  7,870.    Elapsed: 0:15:37.\n",
      "  Batch 7,440  of  7,870.    Elapsed: 0:15:42.\n",
      "  Batch 7,480  of  7,870.    Elapsed: 0:15:47.\n",
      "  Batch 7,520  of  7,870.    Elapsed: 0:15:53.\n",
      "  Batch 7,560  of  7,870.    Elapsed: 0:15:58.\n",
      "  Batch 7,600  of  7,870.    Elapsed: 0:16:03.\n",
      "  Batch 7,640  of  7,870.    Elapsed: 0:16:08.\n",
      "  Batch 7,680  of  7,870.    Elapsed: 0:16:14.\n",
      "  Batch 7,720  of  7,870.    Elapsed: 0:16:19.\n",
      "  Batch 7,760  of  7,870.    Elapsed: 0:16:24.\n",
      "  Batch 7,800  of  7,870.    Elapsed: 0:16:29.\n",
      "  Batch 7,840  of  7,870.    Elapsed: 0:16:34.\n",
      "\n",
      "  Average training loss: 0.49\n",
      "  Training epcoh took: 0:16:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation took: 0:00:29\n",
      "\n",
      "======== Epoch 5 / 15 ========\n",
      "Training...\n",
      "  Batch    40  of  7,870.    Elapsed: 0:00:05.\n",
      "  Batch    80  of  7,870.    Elapsed: 0:00:11.\n",
      "  Batch   120  of  7,870.    Elapsed: 0:00:16.\n",
      "  Batch   160  of  7,870.    Elapsed: 0:00:21.\n",
      "  Batch   200  of  7,870.    Elapsed: 0:00:26.\n",
      "  Batch   240  of  7,870.    Elapsed: 0:00:31.\n",
      "  Batch   280  of  7,870.    Elapsed: 0:00:37.\n",
      "  Batch   320  of  7,870.    Elapsed: 0:00:42.\n",
      "  Batch   360  of  7,870.    Elapsed: 0:00:47.\n",
      "  Batch   400  of  7,870.    Elapsed: 0:00:52.\n",
      "  Batch   440  of  7,870.    Elapsed: 0:00:58.\n",
      "  Batch   480  of  7,870.    Elapsed: 0:01:03.\n",
      "  Batch   520  of  7,870.    Elapsed: 0:01:08.\n",
      "  Batch   560  of  7,870.    Elapsed: 0:01:13.\n",
      "  Batch   600  of  7,870.    Elapsed: 0:01:19.\n",
      "  Batch   640  of  7,870.    Elapsed: 0:01:24.\n",
      "  Batch   680  of  7,870.    Elapsed: 0:01:29.\n",
      "  Batch   720  of  7,870.    Elapsed: 0:01:34.\n",
      "  Batch   760  of  7,870.    Elapsed: 0:01:40.\n",
      "  Batch   800  of  7,870.    Elapsed: 0:01:45.\n",
      "  Batch   840  of  7,870.    Elapsed: 0:01:50.\n",
      "  Batch   880  of  7,870.    Elapsed: 0:01:55.\n",
      "  Batch   920  of  7,870.    Elapsed: 0:02:00.\n",
      "  Batch   960  of  7,870.    Elapsed: 0:02:06.\n",
      "  Batch 1,000  of  7,870.    Elapsed: 0:02:11.\n",
      "  Batch 1,040  of  7,870.    Elapsed: 0:02:16.\n",
      "  Batch 1,080  of  7,870.    Elapsed: 0:02:21.\n",
      "  Batch 1,120  of  7,870.    Elapsed: 0:02:27.\n",
      "  Batch 1,160  of  7,870.    Elapsed: 0:02:32.\n",
      "  Batch 1,200  of  7,870.    Elapsed: 0:02:37.\n",
      "  Batch 1,240  of  7,870.    Elapsed: 0:02:42.\n",
      "  Batch 1,280  of  7,870.    Elapsed: 0:02:48.\n",
      "  Batch 1,320  of  7,870.    Elapsed: 0:02:53.\n",
      "  Batch 1,360  of  7,870.    Elapsed: 0:02:58.\n",
      "  Batch 1,400  of  7,870.    Elapsed: 0:03:03.\n",
      "  Batch 1,440  of  7,870.    Elapsed: 0:03:08.\n",
      "  Batch 1,480  of  7,870.    Elapsed: 0:03:14.\n",
      "  Batch 1,520  of  7,870.    Elapsed: 0:03:19.\n",
      "  Batch 1,560  of  7,870.    Elapsed: 0:03:24.\n",
      "  Batch 1,600  of  7,870.    Elapsed: 0:03:29.\n",
      "  Batch 1,640  of  7,870.    Elapsed: 0:03:35.\n",
      "  Batch 1,680  of  7,870.    Elapsed: 0:03:40.\n",
      "  Batch 1,720  of  7,870.    Elapsed: 0:03:45.\n",
      "  Batch 1,760  of  7,870.    Elapsed: 0:03:50.\n",
      "  Batch 1,800  of  7,870.    Elapsed: 0:03:56.\n",
      "  Batch 1,840  of  7,870.    Elapsed: 0:04:01.\n",
      "  Batch 1,880  of  7,870.    Elapsed: 0:04:06.\n",
      "  Batch 1,920  of  7,870.    Elapsed: 0:04:11.\n",
      "  Batch 1,960  of  7,870.    Elapsed: 0:04:16.\n",
      "  Batch 2,000  of  7,870.    Elapsed: 0:04:22.\n",
      "  Batch 2,040  of  7,870.    Elapsed: 0:04:27.\n",
      "  Batch 2,080  of  7,870.    Elapsed: 0:04:32.\n",
      "  Batch 2,120  of  7,870.    Elapsed: 0:04:37.\n",
      "  Batch 2,160  of  7,870.    Elapsed: 0:04:43.\n",
      "  Batch 2,200  of  7,870.    Elapsed: 0:04:48.\n",
      "  Batch 2,240  of  7,870.    Elapsed: 0:04:53.\n",
      "  Batch 2,280  of  7,870.    Elapsed: 0:04:58.\n",
      "  Batch 2,320  of  7,870.    Elapsed: 0:05:03.\n",
      "  Batch 2,360  of  7,870.    Elapsed: 0:05:09.\n",
      "  Batch 2,400  of  7,870.    Elapsed: 0:05:14.\n",
      "  Batch 2,440  of  7,870.    Elapsed: 0:05:19.\n",
      "  Batch 2,480  of  7,870.    Elapsed: 0:05:24.\n",
      "  Batch 2,520  of  7,870.    Elapsed: 0:05:30.\n",
      "  Batch 2,560  of  7,870.    Elapsed: 0:05:35.\n",
      "  Batch 2,600  of  7,870.    Elapsed: 0:05:40.\n",
      "  Batch 2,640  of  7,870.    Elapsed: 0:05:45.\n",
      "  Batch 2,680  of  7,870.    Elapsed: 0:05:50.\n",
      "  Batch 2,720  of  7,870.    Elapsed: 0:05:56.\n",
      "  Batch 2,760  of  7,870.    Elapsed: 0:06:01.\n",
      "  Batch 2,800  of  7,870.    Elapsed: 0:06:06.\n",
      "  Batch 2,840  of  7,870.    Elapsed: 0:06:11.\n",
      "  Batch 2,880  of  7,870.    Elapsed: 0:06:17.\n",
      "  Batch 2,920  of  7,870.    Elapsed: 0:06:22.\n",
      "  Batch 2,960  of  7,870.    Elapsed: 0:06:27.\n",
      "  Batch 3,000  of  7,870.    Elapsed: 0:06:32.\n",
      "  Batch 3,040  of  7,870.    Elapsed: 0:06:37.\n",
      "  Batch 3,080  of  7,870.    Elapsed: 0:06:43.\n",
      "  Batch 3,120  of  7,870.    Elapsed: 0:06:48.\n",
      "  Batch 3,160  of  7,870.    Elapsed: 0:06:53.\n",
      "  Batch 3,200  of  7,870.    Elapsed: 0:06:58.\n",
      "  Batch 3,240  of  7,870.    Elapsed: 0:07:04.\n",
      "  Batch 3,280  of  7,870.    Elapsed: 0:07:09.\n",
      "  Batch 3,320  of  7,870.    Elapsed: 0:07:14.\n",
      "  Batch 3,360  of  7,870.    Elapsed: 0:07:19.\n",
      "  Batch 3,400  of  7,870.    Elapsed: 0:07:24.\n",
      "  Batch 3,440  of  7,870.    Elapsed: 0:07:30.\n",
      "  Batch 3,480  of  7,870.    Elapsed: 0:07:35.\n",
      "  Batch 3,520  of  7,870.    Elapsed: 0:07:40.\n",
      "  Batch 3,560  of  7,870.    Elapsed: 0:07:45.\n",
      "  Batch 3,600  of  7,870.    Elapsed: 0:07:51.\n",
      "  Batch 3,640  of  7,870.    Elapsed: 0:07:56.\n",
      "  Batch 3,680  of  7,870.    Elapsed: 0:08:01.\n",
      "  Batch 3,720  of  7,870.    Elapsed: 0:08:06.\n",
      "  Batch 3,760  of  7,870.    Elapsed: 0:08:12.\n",
      "  Batch 3,800  of  7,870.    Elapsed: 0:08:17.\n",
      "  Batch 3,840  of  7,870.    Elapsed: 0:08:22.\n",
      "  Batch 3,880  of  7,870.    Elapsed: 0:08:27.\n",
      "  Batch 3,920  of  7,870.    Elapsed: 0:08:33.\n",
      "  Batch 3,960  of  7,870.    Elapsed: 0:08:38.\n",
      "  Batch 4,000  of  7,870.    Elapsed: 0:08:43.\n",
      "  Batch 4,040  of  7,870.    Elapsed: 0:08:48.\n",
      "  Batch 4,080  of  7,870.    Elapsed: 0:08:54.\n",
      "  Batch 4,120  of  7,870.    Elapsed: 0:08:59.\n",
      "  Batch 4,160  of  7,870.    Elapsed: 0:09:04.\n",
      "  Batch 4,200  of  7,870.    Elapsed: 0:09:09.\n",
      "  Batch 4,240  of  7,870.    Elapsed: 0:09:14.\n",
      "  Batch 4,280  of  7,870.    Elapsed: 0:09:20.\n",
      "  Batch 4,320  of  7,870.    Elapsed: 0:09:25.\n",
      "  Batch 4,360  of  7,870.    Elapsed: 0:09:30.\n",
      "  Batch 4,400  of  7,870.    Elapsed: 0:09:35.\n",
      "  Batch 4,440  of  7,870.    Elapsed: 0:09:41.\n",
      "  Batch 4,480  of  7,870.    Elapsed: 0:09:46.\n",
      "  Batch 4,520  of  7,870.    Elapsed: 0:09:51.\n",
      "  Batch 4,560  of  7,870.    Elapsed: 0:09:56.\n",
      "  Batch 4,600  of  7,870.    Elapsed: 0:10:01.\n",
      "  Batch 4,640  of  7,870.    Elapsed: 0:10:07.\n",
      "  Batch 4,680  of  7,870.    Elapsed: 0:10:12.\n",
      "  Batch 4,720  of  7,870.    Elapsed: 0:10:17.\n",
      "  Batch 4,760  of  7,870.    Elapsed: 0:10:22.\n",
      "  Batch 4,800  of  7,870.    Elapsed: 0:10:27.\n",
      "  Batch 4,840  of  7,870.    Elapsed: 0:10:32.\n",
      "  Batch 4,880  of  7,870.    Elapsed: 0:10:37.\n",
      "  Batch 4,920  of  7,870.    Elapsed: 0:10:42.\n",
      "  Batch 4,960  of  7,870.    Elapsed: 0:10:47.\n",
      "  Batch 5,000  of  7,870.    Elapsed: 0:10:52.\n",
      "  Batch 5,040  of  7,870.    Elapsed: 0:10:57.\n",
      "  Batch 5,080  of  7,870.    Elapsed: 0:11:02.\n",
      "  Batch 5,120  of  7,870.    Elapsed: 0:11:07.\n",
      "  Batch 5,160  of  7,870.    Elapsed: 0:11:12.\n",
      "  Batch 5,200  of  7,870.    Elapsed: 0:11:17.\n",
      "  Batch 5,240  of  7,870.    Elapsed: 0:11:22.\n",
      "  Batch 5,280  of  7,870.    Elapsed: 0:11:27.\n",
      "  Batch 5,320  of  7,870.    Elapsed: 0:11:32.\n",
      "  Batch 5,360  of  7,870.    Elapsed: 0:11:37.\n",
      "  Batch 5,400  of  7,870.    Elapsed: 0:11:42.\n",
      "  Batch 5,440  of  7,870.    Elapsed: 0:11:47.\n",
      "  Batch 5,480  of  7,870.    Elapsed: 0:11:52.\n",
      "  Batch 5,520  of  7,870.    Elapsed: 0:11:57.\n",
      "  Batch 5,560  of  7,870.    Elapsed: 0:12:02.\n",
      "  Batch 5,600  of  7,870.    Elapsed: 0:12:07.\n",
      "  Batch 5,640  of  7,870.    Elapsed: 0:12:12.\n",
      "  Batch 5,680  of  7,870.    Elapsed: 0:12:17.\n",
      "  Batch 5,720  of  7,870.    Elapsed: 0:12:22.\n",
      "  Batch 5,760  of  7,870.    Elapsed: 0:12:27.\n",
      "  Batch 5,800  of  7,870.    Elapsed: 0:12:32.\n",
      "  Batch 5,840  of  7,870.    Elapsed: 0:12:37.\n",
      "  Batch 5,880  of  7,870.    Elapsed: 0:12:42.\n",
      "  Batch 5,920  of  7,870.    Elapsed: 0:12:47.\n",
      "  Batch 5,960  of  7,870.    Elapsed: 0:12:52.\n",
      "  Batch 6,000  of  7,870.    Elapsed: 0:12:57.\n",
      "  Batch 6,040  of  7,870.    Elapsed: 0:13:02.\n",
      "  Batch 6,080  of  7,870.    Elapsed: 0:13:07.\n",
      "  Batch 6,120  of  7,870.    Elapsed: 0:13:12.\n",
      "  Batch 6,160  of  7,870.    Elapsed: 0:13:17.\n",
      "  Batch 6,200  of  7,870.    Elapsed: 0:13:22.\n",
      "  Batch 6,240  of  7,870.    Elapsed: 0:13:27.\n",
      "  Batch 6,280  of  7,870.    Elapsed: 0:13:32.\n",
      "  Batch 6,320  of  7,870.    Elapsed: 0:13:37.\n",
      "  Batch 6,360  of  7,870.    Elapsed: 0:13:42.\n",
      "  Batch 6,400  of  7,870.    Elapsed: 0:13:47.\n",
      "  Batch 6,440  of  7,870.    Elapsed: 0:13:52.\n",
      "  Batch 6,480  of  7,870.    Elapsed: 0:13:58.\n",
      "  Batch 6,520  of  7,870.    Elapsed: 0:14:03.\n",
      "  Batch 6,560  of  7,870.    Elapsed: 0:14:08.\n",
      "  Batch 6,600  of  7,870.    Elapsed: 0:14:13.\n",
      "  Batch 6,640  of  7,870.    Elapsed: 0:14:18.\n",
      "  Batch 6,680  of  7,870.    Elapsed: 0:14:23.\n",
      "  Batch 6,720  of  7,870.    Elapsed: 0:14:28.\n",
      "  Batch 6,760  of  7,870.    Elapsed: 0:14:33.\n",
      "  Batch 6,800  of  7,870.    Elapsed: 0:14:38.\n",
      "  Batch 6,840  of  7,870.    Elapsed: 0:14:44.\n",
      "  Batch 6,880  of  7,870.    Elapsed: 0:14:49.\n",
      "  Batch 6,920  of  7,870.    Elapsed: 0:14:55.\n",
      "  Batch 6,960  of  7,870.    Elapsed: 0:15:00.\n",
      "  Batch 7,000  of  7,870.    Elapsed: 0:15:06.\n",
      "  Batch 7,040  of  7,870.    Elapsed: 0:15:12.\n",
      "  Batch 7,080  of  7,870.    Elapsed: 0:15:17.\n",
      "  Batch 7,120  of  7,870.    Elapsed: 0:15:23.\n",
      "  Batch 7,160  of  7,870.    Elapsed: 0:15:28.\n",
      "  Batch 7,200  of  7,870.    Elapsed: 0:15:34.\n",
      "  Batch 7,240  of  7,870.    Elapsed: 0:15:39.\n",
      "  Batch 7,280  of  7,870.    Elapsed: 0:15:45.\n",
      "  Batch 7,320  of  7,870.    Elapsed: 0:15:50.\n",
      "  Batch 7,360  of  7,870.    Elapsed: 0:15:56.\n",
      "  Batch 7,400  of  7,870.    Elapsed: 0:16:01.\n",
      "  Batch 7,440  of  7,870.    Elapsed: 0:16:07.\n",
      "  Batch 7,480  of  7,870.    Elapsed: 0:16:12.\n",
      "  Batch 7,520  of  7,870.    Elapsed: 0:16:18.\n",
      "  Batch 7,560  of  7,870.    Elapsed: 0:16:24.\n",
      "  Batch 7,600  of  7,870.    Elapsed: 0:16:29.\n",
      "  Batch 7,640  of  7,870.    Elapsed: 0:16:35.\n",
      "  Batch 7,680  of  7,870.    Elapsed: 0:16:40.\n",
      "  Batch 7,720  of  7,870.    Elapsed: 0:16:46.\n",
      "  Batch 7,760  of  7,870.    Elapsed: 0:16:51.\n",
      "  Batch 7,800  of  7,870.    Elapsed: 0:16:57.\n",
      "  Batch 7,840  of  7,870.    Elapsed: 0:17:02.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epcoh took: 0:17:06\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation took: 0:00:29\n",
      "\n",
      "======== Epoch 6 / 15 ========\n",
      "Training...\n",
      "  Batch    40  of  7,870.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  7,870.    Elapsed: 0:00:11.\n",
      "  Batch   120  of  7,870.    Elapsed: 0:00:17.\n",
      "  Batch   160  of  7,870.    Elapsed: 0:00:22.\n",
      "  Batch   200  of  7,870.    Elapsed: 0:00:28.\n",
      "  Batch   240  of  7,870.    Elapsed: 0:00:33.\n",
      "  Batch   280  of  7,870.    Elapsed: 0:00:39.\n",
      "  Batch   320  of  7,870.    Elapsed: 0:00:44.\n",
      "  Batch   360  of  7,870.    Elapsed: 0:00:50.\n",
      "  Batch   400  of  7,870.    Elapsed: 0:00:55.\n",
      "  Batch   440  of  7,870.    Elapsed: 0:01:01.\n",
      "  Batch   480  of  7,870.    Elapsed: 0:01:06.\n",
      "  Batch   520  of  7,870.    Elapsed: 0:01:12.\n",
      "  Batch   560  of  7,870.    Elapsed: 0:01:17.\n",
      "  Batch   600  of  7,870.    Elapsed: 0:01:23.\n",
      "  Batch   640  of  7,870.    Elapsed: 0:01:28.\n",
      "  Batch   680  of  7,870.    Elapsed: 0:01:34.\n",
      "  Batch   720  of  7,870.    Elapsed: 0:01:40.\n",
      "  Batch   760  of  7,870.    Elapsed: 0:01:45.\n",
      "  Batch   800  of  7,870.    Elapsed: 0:01:51.\n",
      "  Batch   840  of  7,870.    Elapsed: 0:01:56.\n",
      "  Batch   880  of  7,870.    Elapsed: 0:02:02.\n",
      "  Batch   920  of  7,870.    Elapsed: 0:02:07.\n",
      "  Batch   960  of  7,870.    Elapsed: 0:02:13.\n",
      "  Batch 1,000  of  7,870.    Elapsed: 0:02:18.\n",
      "  Batch 1,040  of  7,870.    Elapsed: 0:02:24.\n",
      "  Batch 1,080  of  7,870.    Elapsed: 0:02:29.\n",
      "  Batch 1,120  of  7,870.    Elapsed: 0:02:35.\n",
      "  Batch 1,160  of  7,870.    Elapsed: 0:02:40.\n",
      "  Batch 1,200  of  7,870.    Elapsed: 0:02:46.\n",
      "  Batch 1,240  of  7,870.    Elapsed: 0:02:51.\n",
      "  Batch 1,280  of  7,870.    Elapsed: 0:02:57.\n",
      "  Batch 1,320  of  7,870.    Elapsed: 0:03:02.\n",
      "  Batch 1,360  of  7,870.    Elapsed: 0:03:08.\n",
      "  Batch 1,400  of  7,870.    Elapsed: 0:03:13.\n",
      "  Batch 1,440  of  7,870.    Elapsed: 0:03:19.\n",
      "  Batch 1,480  of  7,870.    Elapsed: 0:03:24.\n",
      "  Batch 1,520  of  7,870.    Elapsed: 0:03:30.\n",
      "  Batch 1,560  of  7,870.    Elapsed: 0:03:35.\n",
      "  Batch 1,600  of  7,870.    Elapsed: 0:03:41.\n",
      "  Batch 1,640  of  7,870.    Elapsed: 0:03:46.\n",
      "  Batch 1,680  of  7,870.    Elapsed: 0:03:52.\n",
      "  Batch 1,720  of  7,870.    Elapsed: 0:03:57.\n",
      "  Batch 1,760  of  7,870.    Elapsed: 0:04:03.\n",
      "  Batch 1,800  of  7,870.    Elapsed: 0:04:08.\n",
      "  Batch 1,840  of  7,870.    Elapsed: 0:04:14.\n",
      "  Batch 1,880  of  7,870.    Elapsed: 0:04:19.\n",
      "  Batch 1,920  of  7,870.    Elapsed: 0:04:25.\n",
      "  Batch 1,960  of  7,870.    Elapsed: 0:04:30.\n",
      "  Batch 2,000  of  7,870.    Elapsed: 0:04:36.\n",
      "  Batch 2,040  of  7,870.    Elapsed: 0:04:42.\n",
      "  Batch 2,080  of  7,870.    Elapsed: 0:04:47.\n",
      "  Batch 2,120  of  7,870.    Elapsed: 0:04:52.\n",
      "  Batch 2,160  of  7,870.    Elapsed: 0:04:58.\n",
      "  Batch 2,200  of  7,870.    Elapsed: 0:05:04.\n",
      "  Batch 2,240  of  7,870.    Elapsed: 0:05:09.\n",
      "  Batch 2,280  of  7,870.    Elapsed: 0:05:15.\n",
      "  Batch 2,320  of  7,870.    Elapsed: 0:05:20.\n",
      "  Batch 2,360  of  7,870.    Elapsed: 0:05:26.\n",
      "  Batch 2,400  of  7,870.    Elapsed: 0:05:31.\n",
      "  Batch 2,440  of  7,870.    Elapsed: 0:05:37.\n",
      "  Batch 2,480  of  7,870.    Elapsed: 0:05:42.\n",
      "  Batch 2,520  of  7,870.    Elapsed: 0:05:48.\n",
      "  Batch 2,560  of  7,870.    Elapsed: 0:05:53.\n",
      "  Batch 2,600  of  7,870.    Elapsed: 0:05:59.\n",
      "  Batch 2,640  of  7,870.    Elapsed: 0:06:04.\n",
      "  Batch 2,680  of  7,870.    Elapsed: 0:06:10.\n",
      "  Batch 2,720  of  7,870.    Elapsed: 0:06:15.\n",
      "  Batch 2,760  of  7,870.    Elapsed: 0:06:21.\n",
      "  Batch 2,800  of  7,870.    Elapsed: 0:06:26.\n",
      "  Batch 2,840  of  7,870.    Elapsed: 0:06:32.\n",
      "  Batch 2,880  of  7,870.    Elapsed: 0:06:37.\n",
      "  Batch 2,920  of  7,870.    Elapsed: 0:06:43.\n",
      "  Batch 2,960  of  7,870.    Elapsed: 0:06:48.\n",
      "  Batch 3,000  of  7,870.    Elapsed: 0:06:54.\n",
      "  Batch 3,040  of  7,870.    Elapsed: 0:06:59.\n",
      "  Batch 3,080  of  7,870.    Elapsed: 0:07:05.\n",
      "  Batch 3,120  of  7,870.    Elapsed: 0:07:10.\n",
      "  Batch 3,160  of  7,870.    Elapsed: 0:07:16.\n",
      "  Batch 3,200  of  7,870.    Elapsed: 0:07:21.\n",
      "  Batch 3,240  of  7,870.    Elapsed: 0:07:27.\n",
      "  Batch 3,280  of  7,870.    Elapsed: 0:07:32.\n",
      "  Batch 3,320  of  7,870.    Elapsed: 0:07:38.\n",
      "  Batch 3,360  of  7,870.    Elapsed: 0:07:43.\n",
      "  Batch 3,400  of  7,870.    Elapsed: 0:07:49.\n",
      "  Batch 3,440  of  7,870.    Elapsed: 0:07:54.\n",
      "  Batch 3,480  of  7,870.    Elapsed: 0:08:00.\n",
      "  Batch 3,520  of  7,870.    Elapsed: 0:08:05.\n",
      "  Batch 3,560  of  7,870.    Elapsed: 0:08:11.\n",
      "  Batch 3,600  of  7,870.    Elapsed: 0:08:16.\n",
      "  Batch 3,640  of  7,870.    Elapsed: 0:08:22.\n",
      "  Batch 3,680  of  7,870.    Elapsed: 0:08:27.\n",
      "  Batch 3,720  of  7,870.    Elapsed: 0:08:33.\n",
      "  Batch 3,760  of  7,870.    Elapsed: 0:08:38.\n",
      "  Batch 3,800  of  7,870.    Elapsed: 0:08:44.\n",
      "  Batch 3,840  of  7,870.    Elapsed: 0:08:49.\n",
      "  Batch 3,880  of  7,870.    Elapsed: 0:08:55.\n",
      "  Batch 3,920  of  7,870.    Elapsed: 0:09:00.\n",
      "  Batch 3,960  of  7,870.    Elapsed: 0:09:06.\n",
      "  Batch 4,000  of  7,870.    Elapsed: 0:09:11.\n",
      "  Batch 4,040  of  7,870.    Elapsed: 0:09:17.\n",
      "  Batch 4,080  of  7,870.    Elapsed: 0:09:22.\n",
      "  Batch 4,120  of  7,870.    Elapsed: 0:09:28.\n",
      "  Batch 4,160  of  7,870.    Elapsed: 0:09:33.\n",
      "  Batch 4,200  of  7,870.    Elapsed: 0:09:39.\n",
      "  Batch 4,240  of  7,870.    Elapsed: 0:09:44.\n",
      "  Batch 4,280  of  7,870.    Elapsed: 0:09:50.\n",
      "  Batch 4,320  of  7,870.    Elapsed: 0:09:55.\n",
      "  Batch 4,360  of  7,870.    Elapsed: 0:10:01.\n",
      "  Batch 4,400  of  7,870.    Elapsed: 0:10:06.\n",
      "  Batch 4,440  of  7,870.    Elapsed: 0:10:12.\n",
      "  Batch 4,480  of  7,870.    Elapsed: 0:10:17.\n",
      "  Batch 4,520  of  7,870.    Elapsed: 0:10:23.\n",
      "  Batch 4,560  of  7,870.    Elapsed: 0:10:28.\n",
      "  Batch 4,600  of  7,870.    Elapsed: 0:10:34.\n",
      "  Batch 4,640  of  7,870.    Elapsed: 0:10:39.\n",
      "  Batch 4,680  of  7,870.    Elapsed: 0:10:45.\n",
      "  Batch 4,720  of  7,870.    Elapsed: 0:10:50.\n",
      "  Batch 4,760  of  7,870.    Elapsed: 0:10:56.\n",
      "  Batch 4,800  of  7,870.    Elapsed: 0:11:01.\n",
      "  Batch 4,840  of  7,870.    Elapsed: 0:11:07.\n",
      "  Batch 4,880  of  7,870.    Elapsed: 0:11:12.\n",
      "  Batch 4,920  of  7,870.    Elapsed: 0:11:18.\n",
      "  Batch 4,960  of  7,870.    Elapsed: 0:11:23.\n",
      "  Batch 5,000  of  7,870.    Elapsed: 0:11:29.\n",
      "  Batch 5,040  of  7,870.    Elapsed: 0:11:34.\n",
      "  Batch 5,080  of  7,870.    Elapsed: 0:11:40.\n",
      "  Batch 5,120  of  7,870.    Elapsed: 0:11:45.\n",
      "  Batch 5,160  of  7,870.    Elapsed: 0:11:51.\n",
      "  Batch 5,200  of  7,870.    Elapsed: 0:11:56.\n",
      "  Batch 5,240  of  7,870.    Elapsed: 0:12:02.\n",
      "  Batch 5,280  of  7,870.    Elapsed: 0:12:07.\n",
      "  Batch 5,320  of  7,870.    Elapsed: 0:12:13.\n",
      "  Batch 5,360  of  7,870.    Elapsed: 0:12:18.\n",
      "  Batch 5,400  of  7,870.    Elapsed: 0:12:24.\n",
      "  Batch 5,440  of  7,870.    Elapsed: 0:12:29.\n",
      "  Batch 5,480  of  7,870.    Elapsed: 0:12:35.\n",
      "  Batch 5,520  of  7,870.    Elapsed: 0:12:40.\n",
      "  Batch 5,560  of  7,870.    Elapsed: 0:12:46.\n",
      "  Batch 5,600  of  7,870.    Elapsed: 0:12:51.\n",
      "  Batch 5,640  of  7,870.    Elapsed: 0:12:57.\n",
      "  Batch 5,680  of  7,870.    Elapsed: 0:13:02.\n",
      "  Batch 5,720  of  7,870.    Elapsed: 0:13:07.\n",
      "  Batch 5,760  of  7,870.    Elapsed: 0:13:12.\n",
      "  Batch 5,800  of  7,870.    Elapsed: 0:13:18.\n",
      "  Batch 5,840  of  7,870.    Elapsed: 0:13:23.\n",
      "  Batch 5,880  of  7,870.    Elapsed: 0:13:28.\n",
      "  Batch 5,920  of  7,870.    Elapsed: 0:13:33.\n",
      "  Batch 5,960  of  7,870.    Elapsed: 0:13:38.\n",
      "  Batch 6,000  of  7,870.    Elapsed: 0:13:43.\n",
      "  Batch 6,040  of  7,870.    Elapsed: 0:13:49.\n",
      "  Batch 6,080  of  7,870.    Elapsed: 0:13:54.\n",
      "  Batch 6,120  of  7,870.    Elapsed: 0:13:59.\n",
      "  Batch 6,160  of  7,870.    Elapsed: 0:14:04.\n",
      "  Batch 6,200  of  7,870.    Elapsed: 0:14:09.\n",
      "  Batch 6,240  of  7,870.    Elapsed: 0:14:14.\n",
      "  Batch 6,280  of  7,870.    Elapsed: 0:14:20.\n",
      "  Batch 6,320  of  7,870.    Elapsed: 0:14:25.\n",
      "  Batch 6,360  of  7,870.    Elapsed: 0:14:30.\n",
      "  Batch 6,400  of  7,870.    Elapsed: 0:14:35.\n",
      "  Batch 6,440  of  7,870.    Elapsed: 0:14:40.\n",
      "  Batch 6,480  of  7,870.    Elapsed: 0:14:46.\n",
      "  Batch 6,520  of  7,870.    Elapsed: 0:14:51.\n",
      "  Batch 6,560  of  7,870.    Elapsed: 0:14:56.\n",
      "  Batch 6,600  of  7,870.    Elapsed: 0:15:01.\n",
      "  Batch 6,640  of  7,870.    Elapsed: 0:15:06.\n",
      "  Batch 6,680  of  7,870.    Elapsed: 0:15:12.\n",
      "  Batch 6,720  of  7,870.    Elapsed: 0:15:17.\n",
      "  Batch 6,760  of  7,870.    Elapsed: 0:15:22.\n",
      "  Batch 6,800  of  7,870.    Elapsed: 0:15:27.\n",
      "  Batch 6,840  of  7,870.    Elapsed: 0:15:32.\n",
      "  Batch 6,880  of  7,870.    Elapsed: 0:15:38.\n",
      "  Batch 6,920  of  7,870.    Elapsed: 0:15:43.\n",
      "  Batch 6,960  of  7,870.    Elapsed: 0:15:48.\n",
      "  Batch 7,000  of  7,870.    Elapsed: 0:15:53.\n",
      "  Batch 7,040  of  7,870.    Elapsed: 0:15:58.\n",
      "  Batch 7,080  of  7,870.    Elapsed: 0:16:03.\n",
      "  Batch 7,120  of  7,870.    Elapsed: 0:16:09.\n",
      "  Batch 7,160  of  7,870.    Elapsed: 0:16:14.\n",
      "  Batch 7,200  of  7,870.    Elapsed: 0:16:19.\n",
      "  Batch 7,240  of  7,870.    Elapsed: 0:16:24.\n",
      "  Batch 7,280  of  7,870.    Elapsed: 0:16:29.\n",
      "  Batch 7,320  of  7,870.    Elapsed: 0:16:35.\n",
      "  Batch 7,360  of  7,870.    Elapsed: 0:16:40.\n",
      "  Batch 7,400  of  7,870.    Elapsed: 0:16:45.\n",
      "  Batch 7,440  of  7,870.    Elapsed: 0:16:50.\n",
      "  Batch 7,480  of  7,870.    Elapsed: 0:16:55.\n",
      "  Batch 7,520  of  7,870.    Elapsed: 0:17:00.\n",
      "  Batch 7,560  of  7,870.    Elapsed: 0:17:06.\n",
      "  Batch 7,600  of  7,870.    Elapsed: 0:17:11.\n",
      "  Batch 7,640  of  7,870.    Elapsed: 0:17:16.\n",
      "  Batch 7,680  of  7,870.    Elapsed: 0:17:21.\n",
      "  Batch 7,720  of  7,870.    Elapsed: 0:17:26.\n",
      "  Batch 7,760  of  7,870.    Elapsed: 0:17:31.\n",
      "  Batch 7,800  of  7,870.    Elapsed: 0:17:37.\n",
      "  Batch 7,840  of  7,870.    Elapsed: 0:17:42.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epcoh took: 0:17:46\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.98\n",
      "  Validation took: 0:00:29\n",
      "\n",
      "======== Epoch 7 / 15 ========\n",
      "Training...\n",
      "  Batch    40  of  7,870.    Elapsed: 0:00:05.\n",
      "  Batch    80  of  7,870.    Elapsed: 0:00:10.\n",
      "  Batch   120  of  7,870.    Elapsed: 0:00:16.\n",
      "  Batch   160  of  7,870.    Elapsed: 0:00:21.\n",
      "  Batch   200  of  7,870.    Elapsed: 0:00:26.\n",
      "  Batch   240  of  7,870.    Elapsed: 0:00:31.\n",
      "  Batch   280  of  7,870.    Elapsed: 0:00:37.\n",
      "  Batch   320  of  7,870.    Elapsed: 0:00:42.\n",
      "  Batch   360  of  7,870.    Elapsed: 0:00:47.\n",
      "  Batch   400  of  7,870.    Elapsed: 0:00:52.\n",
      "  Batch   440  of  7,870.    Elapsed: 0:00:57.\n",
      "  Batch   480  of  7,870.    Elapsed: 0:01:02.\n",
      "  Batch   520  of  7,870.    Elapsed: 0:01:08.\n",
      "  Batch   560  of  7,870.    Elapsed: 0:01:13.\n",
      "  Batch   600  of  7,870.    Elapsed: 0:01:18.\n",
      "  Batch   640  of  7,870.    Elapsed: 0:01:23.\n",
      "  Batch   680  of  7,870.    Elapsed: 0:01:28.\n",
      "  Batch   720  of  7,870.    Elapsed: 0:01:34.\n",
      "  Batch   760  of  7,870.    Elapsed: 0:01:39.\n",
      "  Batch   800  of  7,870.    Elapsed: 0:01:44.\n",
      "  Batch   840  of  7,870.    Elapsed: 0:01:49.\n",
      "  Batch   880  of  7,870.    Elapsed: 0:01:54.\n",
      "  Batch   920  of  7,870.    Elapsed: 0:01:59.\n",
      "  Batch   960  of  7,870.    Elapsed: 0:02:05.\n",
      "  Batch 1,000  of  7,870.    Elapsed: 0:02:10.\n",
      "  Batch 1,040  of  7,870.    Elapsed: 0:02:15.\n",
      "  Batch 1,080  of  7,870.    Elapsed: 0:02:20.\n",
      "  Batch 1,120  of  7,870.    Elapsed: 0:02:25.\n",
      "  Batch 1,160  of  7,870.    Elapsed: 0:02:31.\n",
      "  Batch 1,200  of  7,870.    Elapsed: 0:02:36.\n",
      "  Batch 1,240  of  7,870.    Elapsed: 0:02:41.\n",
      "  Batch 1,280  of  7,870.    Elapsed: 0:02:46.\n",
      "  Batch 1,320  of  7,870.    Elapsed: 0:02:51.\n",
      "  Batch 1,360  of  7,870.    Elapsed: 0:02:56.\n",
      "  Batch 1,400  of  7,870.    Elapsed: 0:03:01.\n",
      "  Batch 1,440  of  7,870.    Elapsed: 0:03:07.\n",
      "  Batch 1,480  of  7,870.    Elapsed: 0:03:12.\n",
      "  Batch 1,520  of  7,870.    Elapsed: 0:03:17.\n",
      "  Batch 1,560  of  7,870.    Elapsed: 0:03:22.\n",
      "  Batch 1,600  of  7,870.    Elapsed: 0:03:27.\n",
      "  Batch 1,640  of  7,870.    Elapsed: 0:03:33.\n",
      "  Batch 1,680  of  7,870.    Elapsed: 0:03:38.\n",
      "  Batch 1,720  of  7,870.    Elapsed: 0:03:43.\n",
      "  Batch 1,760  of  7,870.    Elapsed: 0:03:48.\n",
      "  Batch 1,800  of  7,870.    Elapsed: 0:03:53.\n",
      "  Batch 1,840  of  7,870.    Elapsed: 0:03:59.\n",
      "  Batch 1,880  of  7,870.    Elapsed: 0:04:04.\n",
      "  Batch 1,920  of  7,870.    Elapsed: 0:04:09.\n",
      "  Batch 1,960  of  7,870.    Elapsed: 0:04:14.\n",
      "  Batch 2,000  of  7,870.    Elapsed: 0:04:19.\n",
      "  Batch 2,040  of  7,870.    Elapsed: 0:04:24.\n",
      "  Batch 2,080  of  7,870.    Elapsed: 0:04:30.\n",
      "  Batch 2,120  of  7,870.    Elapsed: 0:04:35.\n",
      "  Batch 2,160  of  7,870.    Elapsed: 0:04:40.\n",
      "  Batch 2,200  of  7,870.    Elapsed: 0:04:45.\n",
      "  Batch 2,240  of  7,870.    Elapsed: 0:04:50.\n",
      "  Batch 2,280  of  7,870.    Elapsed: 0:04:55.\n",
      "  Batch 2,320  of  7,870.    Elapsed: 0:05:00.\n",
      "  Batch 2,360  of  7,870.    Elapsed: 0:05:06.\n",
      "  Batch 2,400  of  7,870.    Elapsed: 0:05:11.\n",
      "  Batch 2,440  of  7,870.    Elapsed: 0:05:16.\n",
      "  Batch 2,480  of  7,870.    Elapsed: 0:05:21.\n",
      "  Batch 2,520  of  7,870.    Elapsed: 0:05:27.\n",
      "  Batch 2,560  of  7,870.    Elapsed: 0:05:32.\n",
      "  Batch 2,600  of  7,870.    Elapsed: 0:05:37.\n",
      "  Batch 2,640  of  7,870.    Elapsed: 0:05:42.\n",
      "  Batch 2,680  of  7,870.    Elapsed: 0:05:47.\n",
      "  Batch 2,720  of  7,870.    Elapsed: 0:05:52.\n",
      "  Batch 2,760  of  7,870.    Elapsed: 0:05:58.\n",
      "  Batch 2,800  of  7,870.    Elapsed: 0:06:03.\n",
      "  Batch 2,840  of  7,870.    Elapsed: 0:06:08.\n",
      "  Batch 2,880  of  7,870.    Elapsed: 0:06:13.\n",
      "  Batch 2,920  of  7,870.    Elapsed: 0:06:18.\n",
      "  Batch 2,960  of  7,870.    Elapsed: 0:06:23.\n",
      "  Batch 3,000  of  7,870.    Elapsed: 0:06:29.\n",
      "  Batch 3,040  of  7,870.    Elapsed: 0:06:34.\n",
      "  Batch 3,080  of  7,870.    Elapsed: 0:06:39.\n",
      "  Batch 3,120  of  7,870.    Elapsed: 0:06:44.\n",
      "  Batch 3,160  of  7,870.    Elapsed: 0:06:49.\n",
      "  Batch 3,200  of  7,870.    Elapsed: 0:06:55.\n",
      "  Batch 3,240  of  7,870.    Elapsed: 0:07:00.\n",
      "  Batch 3,280  of  7,870.    Elapsed: 0:07:05.\n",
      "  Batch 3,320  of  7,870.    Elapsed: 0:07:10.\n",
      "  Batch 3,360  of  7,870.    Elapsed: 0:07:15.\n",
      "  Batch 3,400  of  7,870.    Elapsed: 0:07:20.\n",
      "  Batch 3,440  of  7,870.    Elapsed: 0:07:26.\n",
      "  Batch 3,480  of  7,870.    Elapsed: 0:07:31.\n",
      "  Batch 3,520  of  7,870.    Elapsed: 0:07:36.\n",
      "  Batch 3,560  of  7,870.    Elapsed: 0:07:41.\n",
      "  Batch 3,600  of  7,870.    Elapsed: 0:07:46.\n",
      "  Batch 3,640  of  7,870.    Elapsed: 0:07:51.\n",
      "  Batch 3,680  of  7,870.    Elapsed: 0:07:57.\n",
      "  Batch 3,720  of  7,870.    Elapsed: 0:08:02.\n",
      "  Batch 3,760  of  7,870.    Elapsed: 0:08:07.\n",
      "  Batch 3,800  of  7,870.    Elapsed: 0:08:12.\n",
      "  Batch 3,840  of  7,870.    Elapsed: 0:08:17.\n",
      "  Batch 3,880  of  7,870.    Elapsed: 0:08:22.\n",
      "  Batch 3,920  of  7,870.    Elapsed: 0:08:28.\n",
      "  Batch 3,960  of  7,870.    Elapsed: 0:08:33.\n",
      "  Batch 4,000  of  7,870.    Elapsed: 0:08:38.\n",
      "  Batch 4,040  of  7,870.    Elapsed: 0:08:43.\n",
      "  Batch 4,080  of  7,870.    Elapsed: 0:08:49.\n",
      "  Batch 4,120  of  7,870.    Elapsed: 0:08:54.\n",
      "  Batch 4,160  of  7,870.    Elapsed: 0:08:59.\n",
      "  Batch 4,200  of  7,870.    Elapsed: 0:09:04.\n",
      "  Batch 4,240  of  7,870.    Elapsed: 0:09:09.\n",
      "  Batch 4,280  of  7,870.    Elapsed: 0:09:14.\n",
      "  Batch 4,320  of  7,870.    Elapsed: 0:09:20.\n",
      "  Batch 4,360  of  7,870.    Elapsed: 0:09:25.\n",
      "  Batch 4,400  of  7,870.    Elapsed: 0:09:30.\n",
      "  Batch 4,440  of  7,870.    Elapsed: 0:09:35.\n",
      "  Batch 4,480  of  7,870.    Elapsed: 0:09:40.\n",
      "  Batch 4,520  of  7,870.    Elapsed: 0:09:46.\n",
      "  Batch 4,560  of  7,870.    Elapsed: 0:09:51.\n",
      "  Batch 4,600  of  7,870.    Elapsed: 0:09:56.\n",
      "  Batch 4,640  of  7,870.    Elapsed: 0:10:01.\n",
      "  Batch 4,680  of  7,870.    Elapsed: 0:10:07.\n",
      "  Batch 4,720  of  7,870.    Elapsed: 0:10:12.\n",
      "  Batch 4,760  of  7,870.    Elapsed: 0:10:17.\n",
      "  Batch 4,800  of  7,870.    Elapsed: 0:10:22.\n",
      "  Batch 4,840  of  7,870.    Elapsed: 0:10:27.\n",
      "  Batch 4,880  of  7,870.    Elapsed: 0:10:33.\n",
      "  Batch 4,920  of  7,870.    Elapsed: 0:10:38.\n",
      "  Batch 4,960  of  7,870.    Elapsed: 0:10:43.\n",
      "  Batch 5,000  of  7,870.    Elapsed: 0:10:48.\n",
      "  Batch 5,040  of  7,870.    Elapsed: 0:10:54.\n",
      "  Batch 5,080  of  7,870.    Elapsed: 0:10:59.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "\n",
    "# seed value to make it reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = PATH+'/{0}/CambemBERT/'.format(DOMAIN)\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "cambertTokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_data/elipsis_test/data_preprocessed/source_concatenate_without_special_char', 'r') as f:\n",
    "    sentences = [sent.strip() for sent in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_data/elipsis_test/data_preprocessed/core_sentences', 'r') as f:\n",
    "    labels = [canonical2index.get(sent.strip(), -1) for sent in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(len(sentences)))\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = cambertTokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, classification_report\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  \n",
    "  # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flat_true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-3c2b4518c220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_true_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'flat_true_labels' is not defined"
     ]
    }
   ],
   "source": [
    "a = list(filter(lambda prediction: prediction[0] > 0, zip(flat_true_labels, flat_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, predictions = list(zip(*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
