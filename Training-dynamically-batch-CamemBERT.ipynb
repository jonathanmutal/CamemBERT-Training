{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.2.0 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (1.2.0+cu92)\n",
      "Requirement already satisfied: torchvision==0.4.0 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (0.4.0+cu92)\n",
      "Requirement already satisfied: numpy in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from torch==1.2.0) (1.17.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from torchvision==0.4.0) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from torchvision==0.4.0) (7.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/export/data/swisspost/enviroments/word_embeddings/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: seaborn in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (0.10.0)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from seaborn) (3.1.1)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from seaborn) (1.3.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from seaborn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from seaborn) (1.17.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from pandas>=0.22.0->seaborn) (2019.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from cycler>=0.10->matplotlib>=2.1.2->seaborn) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.2->seaborn) (41.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/export/data/swisspost/enviroments/word_embeddings/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.2.0 torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification, CamembertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_update_interval(total_iters, num_desired_updates):\n",
    "    '''\n",
    "    This function will try to pick an intelligent progress update interval \n",
    "    based on the magnitude of the total iterations.\n",
    "\n",
    "    Parameters:\n",
    "      `total_iters` - The number of iterations in the for-loop.\n",
    "      `num_desired_updates` - How many times we want to see an update over the \n",
    "                              course of the for-loop.\n",
    "    '''\n",
    "    # Divide the total iterations by the desired number of updates. Most likely\n",
    "    # this will be some ugly number.\n",
    "    exact_interval = total_iters / num_desired_updates\n",
    "\n",
    "    # The `round` function has the ability to round down a number to, e.g., the\n",
    "    # nearest thousandth: round(exact_interval, -3)\n",
    "    #\n",
    "    # To determine the magnitude to round to, find the magnitude of the total,\n",
    "    # and then go one magnitude below that.\n",
    "\n",
    "    # Get the order of magnitude of the total.\n",
    "    order_of_mag = len(str(total_iters)) - 1\n",
    "\n",
    "    # Our update interval should be rounded to an order of magnitude smaller. \n",
    "    round_mag = order_of_mag - 1\n",
    "\n",
    "    # Round down and cast to an int.\n",
    "    update_interval = int(round(exact_interval, -round_mag))\n",
    "\n",
    "    # Don't allow the interval to be zero!\n",
    "    if update_interval == 0:\n",
    "        update_interval = 1\n",
    "\n",
    "    return update_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(##)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.variation.isna()]\n",
    "df\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "preprocess = lambda sent: re.sub(r'\\s+', ' ', re.sub('-', ' ', re.sub(\"'\", \"' \",  sent.strip())))\n",
    "preprocess_canonical = lambda sent: re.sub(r'\\s+', ' ', sent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/data/swisspost/enviroments/word_embeddings/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df.variations = df.variation.apply(preprocess)\n",
    "df.canonical = df.canonical.apply(preprocess_canonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_model(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2canonical = df.canonical.unique()\n",
    "canonical2index = dict([(canonical, i) for (i, canonical) in enumerate(index2canonical)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isdir(os.path.join('.', 'dynamic-example')):\n",
    "    os.mkdir(os.path.join('.', 'dynamic-example'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(index2canonical, os.path.join('.', 'dynamic-example', 'index2canonical'))\n",
    "save_model(canonical2index, os.path.join('.', 'dynamic-example', 'canonical2index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.canonical.apply(lambda sent: canonical2index[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = df.variation.values\n",
    "train_label = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will take the max from the tokenizer\n",
    "max_len = max([len(tokenizer.tokenize(sentence, add_special_tokens=True)) for sentence in train_sentences])\n",
    "# for the special tokens that BERT requires\n",
    "max_len += 2\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_smart_batches(text_samples, labels, batch_size=16):\n",
    "    '''\n",
    "    This function combines all of the required steps to prepare batches.\n",
    "    Tokenize--select batches--padded.\n",
    "    '''\n",
    "\n",
    "    print('Creating Smart Batches from {:,} examples with batch size {:,}...\\n'.format(len(text_samples), batch_size))\n",
    "\n",
    "    # =========================\n",
    "    #   Tokenize & Truncate\n",
    "    # =========================\n",
    "\n",
    "    full_input_ids = []\n",
    "\n",
    "    # Tokenize all training examples\n",
    "    print('Tokenizing {:,} samples...'.format(len(labels)))\n",
    "\n",
    "    # Choose an interval on which to print progress updates.\n",
    "    update_interval = good_update_interval(total_iters=len(labels), num_desired_updates=10)\n",
    "\n",
    "    # For each training example...\n",
    "    for text in text_samples:\n",
    "        \n",
    "        # Report progress.\n",
    "        if ((len(full_input_ids) % update_interval) == 0):\n",
    "            print('  Tokenized {:,} samples.'.format(len(full_input_ids)))\n",
    "\n",
    "        # Tokenize the sample.\n",
    "        input_ids = tokenizer.encode(text=text,              # Text to encode.\n",
    "                                    add_special_tokens=True, # Do add specials.\n",
    "                                    max_length=max_len,      # Do Truncate!\n",
    "                                    truncation=True,         # Do Truncate!\n",
    "                                    padding=False)           # DO NOT pad.\n",
    "                                    \n",
    "        # Add the tokenized result to our list.\n",
    "        full_input_ids.append(input_ids)\n",
    "        \n",
    "    print('DONE.')\n",
    "    print('{:>10,} samples\\n'.format(len(full_input_ids)))\n",
    "\n",
    "    # =========================\n",
    "    #      Select Batches\n",
    "    # =========================    \n",
    "\n",
    "    # Sort the two lists together by the length of the input sequence.\n",
    "    samples = sorted(zip(full_input_ids, labels), key=lambda x: len(x[0]))\n",
    "\n",
    "    print('{:>10,} samples after sorting\\n'.format(len(samples)))\n",
    "\n",
    "    import random\n",
    "\n",
    "    # List of batches that we'll construct.\n",
    "    batch_ordered_sentences = []\n",
    "    batch_ordered_labels = []\n",
    "\n",
    "    print('Creating batches of size {:}...'.format(batch_size))\n",
    "\n",
    "    # Choose an interval on which to print progress updates.\n",
    "    update_interval = good_update_interval(total_iters=len(samples), num_desired_updates=10)\n",
    "    \n",
    "    # Loop over all of the input samples...    \n",
    "    while len(samples) > 0:\n",
    "        \n",
    "        # Report progress.\n",
    "        if ((len(batch_ordered_sentences) % update_interval) == 0 \\\n",
    "            and not len(batch_ordered_sentences) == 0):\n",
    "            print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "        # `to_take` is our actual batch size. It will be `batch_size` until \n",
    "        # we get to the last batch, which may be smaller. \n",
    "        to_take = min(batch_size, len(samples))\n",
    "\n",
    "        # Pick a random index in the list of remaining samples to start\n",
    "        # our batch at.\n",
    "        select = random.randint(0, len(samples) - to_take)\n",
    "\n",
    "        # Select a contiguous batch of samples starting at `select`.\n",
    "        #print(\"Selecting batch from {:} to {:}\".format(select, select+to_take))\n",
    "        batch = samples[select:(select + to_take)]\n",
    "\n",
    "        #print(\"Batch length:\", len(batch))\n",
    "\n",
    "        # Each sample is a tuple--split them apart to create a separate list of \n",
    "        # sequences and a list of labels for this batch.\n",
    "        batch_ordered_sentences.append([s[0] for s in batch])\n",
    "        batch_ordered_labels.append([s[1] for s in batch])\n",
    "\n",
    "        # Remove these samples from the list.\n",
    "        del samples[select:select + to_take]\n",
    "\n",
    "    print('\\n  DONE - Selected {:,} batches.\\n'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "    # =========================\n",
    "    #        Add Padding\n",
    "    # =========================    \n",
    "\n",
    "    print('Padding out sequences within each batch...')\n",
    "\n",
    "    py_inputs = []\n",
    "    py_attn_masks = []\n",
    "    py_labels = []\n",
    "\n",
    "    # For each batch...\n",
    "    for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n",
    "\n",
    "        # New version of the batch, this time with padded sequences and now with\n",
    "        # attention masks defined.\n",
    "        batch_padded_inputs = []\n",
    "        batch_attn_masks = []\n",
    "        \n",
    "        # First, find the longest sample in the batch. \n",
    "        # Note that the sequences do currently include the special tokens!\n",
    "        max_size = max([len(sen) for sen in batch_inputs])\n",
    "\n",
    "        # For each input in this batch...\n",
    "        for sen in batch_inputs:\n",
    "            \n",
    "            # How many pad tokens do we need to add?\n",
    "            num_pads = max_size - len(sen)\n",
    "\n",
    "            # Add `num_pads` padding tokens to the end of the sequence.\n",
    "            padded_input = sen + [tokenizer.pad_token_id]*num_pads\n",
    "\n",
    "            # Define the attention mask--it's just a `1` for every real token\n",
    "            # and a `0` for every padding token.\n",
    "            attn_mask = [1] * len(sen) + [0] * num_pads\n",
    "\n",
    "            # Add the padded results to the batch.\n",
    "            batch_padded_inputs.append(padded_input)\n",
    "            batch_attn_masks.append(attn_mask)\n",
    "\n",
    "        # Our batch has been padded, so we need to save this updated batch.\n",
    "        # We also need the inputs to be PyTorch tensors, so we'll do that here.\n",
    "        # Todo - Michael's code specified \"dtype=torch.long\"\n",
    "        py_inputs.append(torch.tensor(batch_padded_inputs))\n",
    "        py_attn_masks.append(torch.tensor(batch_attn_masks))\n",
    "        py_labels.append(torch.tensor(batch_labels))\n",
    "    \n",
    "    print('  DONE.')\n",
    "\n",
    "    # Return the smart-batched dataset!\n",
    "    return (py_inputs, py_attn_masks, py_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Smart Batches from 213,529 examples with batch size 32...\n",
      "\n",
      "Tokenizing 213,529 samples...\n",
      "  Tokenized 0 samples.\n",
      "  Tokenized 20,000 samples.\n",
      "  Tokenized 40,000 samples.\n",
      "  Tokenized 60,000 samples.\n",
      "  Tokenized 80,000 samples.\n",
      "  Tokenized 100,000 samples.\n",
      "  Tokenized 120,000 samples.\n",
      "  Tokenized 140,000 samples.\n",
      "  Tokenized 160,000 samples.\n",
      "  Tokenized 180,000 samples.\n",
      "  Tokenized 200,000 samples.\n",
      "DONE.\n",
      "   213,529 samples\n",
      "\n",
      "   213,529 samples after sorting\n",
      "\n",
      "Creating batches of size 32...\n",
      "\n",
      "  DONE - Selected 6,673 batches.\n",
      "\n",
      "Padding out sequences within each batch...\n",
      "  DONE.\n"
     ]
    }
   ],
   "source": [
    "py_inputs, py_attn_masks, py_labels = make_smart_batches(train_sentences, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-training model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config type: <class 'transformers.configuration_camembert.CamembertConfig'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# Load the Config object, with an output configured for classification.\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name_or_path='camembert-base',\n",
    "                                    num_labels=len(index2canonical))\n",
    "\n",
    "print('Config type:', str(type(config)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model type: <class 'transformers.modeling_camembert.CamembertForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model for classification, passing in the `config` from\n",
    "# above.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path='camembert-base',\n",
    "    config=config)\n",
    "\n",
    "print('\\nModel type:', str(type(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model to GPU...\n",
      "  GPU: TITAN RTX\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print('\\nLoading model to GPU...')\n",
    "\n",
    "device = torch.device('cuda', 0)\n",
    "\n",
    "print('  GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "desc = model.to(device)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # This is the value Michael used.\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. I chose to train for 1 simply because the training\n",
    "# time is long. More epochs may improve the model's accuracy.\n",
    "epochs = 30\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# Note that it's the number of *batches*, not *samples*!\n",
    "total_steps = len(py_inputs) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 30 ========\n",
      "Training on 6,673 batches...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 321\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Update every `update_interval` batches.\n",
    "update_interval = good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    \n",
    "    if epoch_i > 0:\n",
    "        (py_inputs, py_attn_masks, py_labels) = make_smart_batches(train_sentences, train_labels, batch_size)\n",
    "    \n",
    "    print('Training on {:,} batches...'.format(len(py_inputs)))\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step in range(0, len(py_inputs)):\n",
    "\n",
    "        # Progress update every, e.g., 100 batches.\n",
    "        if step % update_interval == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Calculate the time remaining based on our progress.\n",
    "            steps_per_sec = (time.time() - t0) / step\n",
    "            remaining_sec = steps_per_sec * (len(py_inputs) - step)\n",
    "            remaining = format_time(remaining_sec)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))\n",
    "\n",
    "        # Copy the current training batch to the GPU using the `to` method.\n",
    "        b_input_ids = py_inputs[step].to(device)\n",
    "        b_input_mask = py_attn_masks[step].to(device)\n",
    "        b_labels = py_labels[step].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass.\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The call returns the loss (because we provided labels) and the \n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(py_inputs)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Training Time': training_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
